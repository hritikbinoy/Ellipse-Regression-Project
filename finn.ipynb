{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FINN Optimization Pipeline for Quantized Ellipse Regression Model\n",
    "==================================================================\n",
    "\n",
    "Prerequisites:\n",
    "1. QONNX model exported from QAT training\n",
    "2. FINN installed (finn-base, finn-experimental)\n",
    "3. Docker (optional but recommended for FINN)\n",
    "\n",
    "Steps:\n",
    "1. Load QONNX model\n",
    "2. Apply FINN transformations\n",
    "3. Generate HDL/IP for FPGA deployment\n",
    "4. Performance estimation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b11105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " QONNX imports successful\n",
      " FINN/QONNX transformations available\n",
      "  FINN version: unknown\n",
      "  FINN path: None\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports (Updated for Latest FINN)\n",
    "import os\n",
    "import numpy as np\n",
    "import onnx\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import GiveUniqueNodeNames, GiveReadableTensorNames\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "\n",
    "print(\" QONNX imports successful\")\n",
    "\n",
    "# FINN-specific imports (graceful fallback if not available)\n",
    "FINN_AVAILABLE = False\n",
    "try:\n",
    "    # Try new FINN import paths first\n",
    "    try:\n",
    "        from finn.transformation.streamline import Streamline\n",
    "        from finn.transformation.streamline.reorder import MoveScalarLinearPastInvariants\n",
    "    except ImportError:\n",
    "        # Fallback to alternative paths\n",
    "        from qonnx.transformation.streamline import Streamline\n",
    "        from qonnx.transformation.general import RemoveUnusedTensors\n",
    "    \n",
    "    FINN_AVAILABLE = True\n",
    "    print(\" FINN/QONNX transformations available\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\" Advanced FINN transformations not available: {e}\")\n",
    "    print(\"   Basic QONNX optimization will still work\")\n",
    "\n",
    "# Check what's actually available\n",
    "import sys\n",
    "if 'finn' in sys.modules:\n",
    "    import finn\n",
    "    print(f\"  FINN version: {getattr(finn, '__version__', 'unknown')}\")\n",
    "    print(f\"  FINN path: {finn.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd1ecc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING QONNX MODEL\n",
      "======================================================================\n",
      "\n",
      " Loaded QONNX model: exports/ellipse_qat_model_finn.qonnx\n",
      "  Input: input\n",
      "  Output: output\n",
      "  Total nodes: 180\n",
      "\n",
      "  Node type distribution:\n",
      "    Where: 32\n",
      "    Abs: 25\n",
      "    Mul: 21\n",
      "    Div: 16\n",
      "    Round: 11\n",
      "    Greater: 11\n",
      "    Less: 11\n",
      "    GreaterOrEqual: 10\n",
      "    Clip: 10\n",
      "    Relu: 6\n",
      "    Reshape: 6\n",
      "    ReduceMax: 5\n",
      "    Conv: 4\n",
      "    MaxPool: 4\n",
      "    BatchNormalization: 3\n",
      "    Gemm: 3\n",
      "    Shape: 1\n",
      "    Concat: 1\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load QONNX Model\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING QONNX MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "qonnx_path = \"exports/ellipse_qat_model_finn.qonnx\"\n",
    "\n",
    "if not os.path.exists(qonnx_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"QONNX model not found: {qonnx_path}\\n\"\n",
    "        \"Run the QAT export cell in Model.ipynb first!\"\n",
    "    )\n",
    "\n",
    "# Load model\n",
    "model = ModelWrapper(qonnx_path)\n",
    "print(f\"\\n Loaded QONNX model: {qonnx_path}\")\n",
    "print(f\"  Input: {model.graph.input[0].name}\")\n",
    "print(f\"  Output: {model.graph.output[0].name}\")\n",
    "print(f\"  Total nodes: {len(model.graph.node)}\")\n",
    "\n",
    "# Count node types\n",
    "from collections import Counter\n",
    "node_types = Counter([n.op_type for n in model.graph.node])\n",
    "print(f\"\\n  Node type distribution:\")\n",
    "for op_type, count in sorted(node_types.items(), key=lambda x: -x[1]):\n",
    "    print(f\"    {op_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f373c767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 1: QONNX CLEANUP & OPTIMIZATION\n",
      "======================================================================\n",
      "\n",
      "[1/5] Inferring shapes...\n",
      "   Shape inference complete\n",
      "\n",
      "[2/5] Folding constants...\n",
      "   Constant folding complete\n",
      "\n",
      "[3/5] Assigning unique node names...\n",
      "   Node naming complete\n",
      "\n",
      "[4/5] Creating readable tensor names...\n",
      "   Tensor naming complete\n",
      "\n",
      "[5/5] Running cleanup...\n",
      "   Cleanup complete\n",
      "\n",
      " Cleanup complete\n",
      "  Original nodes: 180\n",
      "  Optimized nodes: 63\n",
      "  Nodes removed: 117\n",
      "\n",
      " Saved: finn_build/ellipse_qonnx_cleaned.onnx\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Apply QONNX Cleanup Transformations\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: QONNX CLEANUP & OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[1/5] Inferring shapes...\")\n",
    "model = model.transform(InferShapes())\n",
    "print(\"   Shape inference complete\")\n",
    "\n",
    "print(\"\\n[2/5] Folding constants...\")\n",
    "model = model.transform(FoldConstants())\n",
    "print(\"   Constant folding complete\")\n",
    "\n",
    "print(\"\\n[3/5] Assigning unique node names...\")\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "print(\"   Node naming complete\")\n",
    "\n",
    "print(\"\\n[4/5] Creating readable tensor names...\")\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "print(\"   Tensor naming complete\")\n",
    "\n",
    "# Save model before cleanup\n",
    "os.makedirs(\"finn_build\", exist_ok=True)\n",
    "temp_path = \"finn_build/ellipse_qonnx_temp.onnx\"\n",
    "model.save(temp_path)\n",
    "\n",
    "print(\"\\n[5/5] Running cleanup...\")\n",
    "# qonnx_cleanup expects a file path, not ModelWrapper\n",
    "cleaned_path = \"finn_build/ellipse_qonnx_cleaned.onnx\"\n",
    "qonnx_cleanup(temp_path, out_file=cleaned_path)\n",
    "print(\"   Cleanup complete\")\n",
    "\n",
    "# Reload the cleaned model\n",
    "model = ModelWrapper(cleaned_path)\n",
    "\n",
    "# Count optimized nodes\n",
    "optimized_nodes = len(model.graph.node)\n",
    "print(f\"\\n Cleanup complete\")\n",
    "print(f\"  Original nodes: {sum(node_types.values())}\")\n",
    "print(f\"  Optimized nodes: {optimized_nodes}\")\n",
    "print(f\"  Nodes removed: {sum(node_types.values()) - optimized_nodes}\")\n",
    "print(f\"\\n Saved: {cleaned_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcbc8908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: FINN STREAMLINING\n",
      "======================================================================\n",
      "\n",
      "[1/4] Applying Streamline transformation...\n",
      "  ‚úì Streamlining complete\n",
      "\n",
      "[2/4] Removing unused tensors...\n",
      "  ‚úì Unused tensors removed\n",
      "\n",
      "[3/4] Applying shape inference after streamlining...\n",
      "  ‚úì Shape inference complete\n",
      "\n",
      "[4/4] Fixing Reshape nodes...\n",
      "  Found 1 Reshape nodes\n",
      "    Reshape_0 (Reshape_0): [ 0 -1] ‚Üí [ 1 -1] ‚úì\n",
      "  ‚úì Fixed 1 Reshape nodes\n",
      "\n",
      "‚úì Streamlined model saved: finn_build/ellipse_qonnx_streamlined.onnx\n",
      "  Current nodes: 63\n",
      "\n",
      "‚úì Optimization pipeline complete\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Apply FINN Streamlining with Automatic Reshape Fixing\n",
    "\n",
    "import onnx\n",
    "import numpy as np\n",
    "from onnx import numpy_helper\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: FINN STREAMLINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Defensive: Ensure we have a ModelWrapper, not ModelProto\n",
    "if not isinstance(model, ModelWrapper):\n",
    "    print(\"‚ö†Ô∏è  Model is not a ModelWrapper, reloading from cleaned path...\")\n",
    "    model = ModelWrapper(cleaned_path)\n",
    "\n",
    "if FINN_AVAILABLE:\n",
    "    try:\n",
    "        print(\"\\n[1/4] Applying Streamline transformation...\")\n",
    "        \n",
    "        # Apply streamline - it returns (model, changed) tuple\n",
    "        streamlined_result = model.transform(Streamline())\n",
    "        \n",
    "        # Check what we got back and wrap if needed\n",
    "        if not isinstance(streamlined_result, ModelWrapper):\n",
    "            print(\"  ‚ö†Ô∏è  Transform returned ModelProto, wrapping back...\")\n",
    "            temp_stream_path = \"finn_build/ellipse_qonnx_temp_streamlined.onnx\"\n",
    "            onnx.save(streamlined_result, temp_stream_path)\n",
    "            model = ModelWrapper(temp_stream_path)\n",
    "        else:\n",
    "            model = streamlined_result\n",
    "        \n",
    "        print(\"  ‚úì Streamlining complete\")\n",
    "        \n",
    "        print(\"\\n[2/4] Removing unused tensors...\")\n",
    "        try:\n",
    "            from qonnx.transformation.general import RemoveUnusedTensors\n",
    "            unused_result = model.transform(RemoveUnusedTensors())\n",
    "            \n",
    "            # Again, ensure we keep a ModelWrapper\n",
    "            if not isinstance(unused_result, ModelWrapper):\n",
    "                print(\"  ‚ö†Ô∏è  Transform returned ModelProto, wrapping back...\")\n",
    "                temp_unused_path = \"finn_build/ellipse_qonnx_temp_unused.onnx\"\n",
    "                onnx.save(unused_result, temp_unused_path)\n",
    "                model = ModelWrapper(temp_unused_path)\n",
    "            else:\n",
    "                model = unused_result\n",
    "            \n",
    "            print(\"  ‚úì Unused tensors removed\")\n",
    "        except ImportError:\n",
    "            print(\"  ‚äò RemoveUnusedTensors not available (skipped)\")\n",
    "        \n",
    "        print(\"\\n[3/4] Applying shape inference after streamlining...\")\n",
    "        model = model.transform(InferShapes())\n",
    "        print(\"  ‚úì Shape inference complete\")\n",
    "        \n",
    "        # Save temporary streamlined model\n",
    "        temp_streamlined_path = \"finn_build/ellipse_qonnx_streamlined_temp.onnx\"\n",
    "        model.save(temp_streamlined_path)\n",
    "        \n",
    "        # ===================================================================\n",
    "        # AUTOMATIC RESHAPE FIXING\n",
    "        # ===================================================================\n",
    "        print(\"\\n[4/4] Fixing Reshape nodes...\")\n",
    "        \n",
    "        # Load as ONNX model for editing\n",
    "        onnx_model = onnx.load(temp_streamlined_path)\n",
    "        graph = onnx_model.graph\n",
    "        \n",
    "        # Find all Reshape nodes\n",
    "        reshape_nodes = [n for n in graph.node if n.op_type == \"Reshape\"]\n",
    "        print(f\"  Found {len(reshape_nodes)} Reshape nodes\")\n",
    "        \n",
    "        fixed_count = 0\n",
    "        \n",
    "        for i, node in enumerate(reshape_nodes):\n",
    "            shape_name = node.input[1]\n",
    "            \n",
    "            # Find the shape initializer\n",
    "            for init in graph.initializer:\n",
    "                if init.name == shape_name:\n",
    "                    shape = numpy_helper.to_array(init)\n",
    "                    \n",
    "                    # Check if shape contains 0 or -1 (problematic)\n",
    "                    if 0 in shape or (np.sum(shape == -1) > 0 and len(shape) > 1):\n",
    "                        print(f\"    Reshape_{i} ({node.name}): {shape} ‚Üí \", end=\"\")\n",
    "                        \n",
    "                        # Check if this is the FINAL reshape (output node)\n",
    "                        is_output = any(node.output[0] == out.name for out in graph.output)\n",
    "                        \n",
    "                        if is_output or \"output\" in node.output[0].lower():\n",
    "                            # Final reshape: should be (1, 5) for ellipse parameters\n",
    "                            new_shape = np.array([1, 5], dtype=np.int64)\n",
    "                        else:\n",
    "                            # Intermediate reshape: replace 0 with 1, keep other dims\n",
    "                            new_shape = np.where(shape == 0, 1, shape)\n",
    "                            # If there's a -1, replace with calculated value\n",
    "                            if -1 in new_shape:\n",
    "                                new_shape = np.array([1, -1], dtype=np.int64)\n",
    "                        \n",
    "                        print(f\"{new_shape} ‚úì\")\n",
    "                        \n",
    "                        # Update the initializer\n",
    "                        init.CopyFrom(numpy_helper.from_array(new_shape, name=shape_name))\n",
    "                        fixed_count += 1\n",
    "        \n",
    "        if fixed_count > 0:\n",
    "            print(f\"  ‚úì Fixed {fixed_count} Reshape nodes\")\n",
    "        else:\n",
    "            print(f\"  ‚úì All Reshape nodes are valid\")\n",
    "        \n",
    "        # Save final fixed model\n",
    "        streamlined_path = \"finn_build/ellipse_qonnx_streamlined.onnx\"\n",
    "        onnx.save(onnx_model, streamlined_path)\n",
    "        \n",
    "        # Reload as ModelWrapper\n",
    "        model = ModelWrapper(streamlined_path)\n",
    "        \n",
    "        print(f\"\\n‚úì Streamlined model saved: {streamlined_path}\")\n",
    "        print(f\"  Current nodes: {len(model.graph.node)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö† Streamlining failed: {e}\")\n",
    "        print(f\"  Error type: {type(e).__name__}\")\n",
    "        import traceback\n",
    "        print(\"\\nFull traceback:\")\n",
    "        traceback.print_exc()\n",
    "        print(\"\\n  Continuing with cleaned model...\")\n",
    "        streamlined_path = cleaned_path\n",
    "        # Reload to ensure we have a valid ModelWrapper\n",
    "        model = ModelWrapper(cleaned_path)\n",
    "else:\n",
    "    print(\"‚äò FINN transformations not available\")\n",
    "    print(\"  Using cleaned QONNX model\")\n",
    "    streamlined_path = cleaned_path\n",
    "\n",
    "print(\"\\n‚úì Optimization pipeline complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ef4ba15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: MODEL VERIFICATION\n",
      "======================================================================\n",
      "‚ö†Ô∏è  Using original streamlined model\n",
      "\n",
      "[1/3] Checking ONNX validity...\n",
      "  ‚úì Model is valid ONNX\n",
      "\n",
      "[2/3] Testing inference with ONNX Runtime...\n",
      "  Model input: 'global_in'\n",
      "  Model output: 'global_out'\n",
      "  ‚úì Inference successful\n",
      "    Input shape: (1, 1, 20, 20)\n",
      "    Output shape: (1, 5)\n",
      "    Output range: [-0.0882, 29.2421]\n",
      "\n",
      "[3/3] Model statistics...\n",
      "  File size: 2.62 MB\n",
      "  Total nodes: 63\n",
      "\n",
      "  Final node type distribution:\n",
      "    Mul: 12\n",
      "    Where: 12\n",
      "    Relu: 6\n",
      "    Round: 6\n",
      "    Greater: 6\n",
      "    Less: 6\n",
      "    Conv: 4\n",
      "    MaxPool: 4\n",
      "    Add: 3\n",
      "    Gemm: 3\n",
      "    Reshape: 1\n",
      "\n",
      "======================================================================\n",
      "‚úì OPTIMIZATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "üìÅ Optimized model: finn_build/ellipse_qonnx_streamlined.onnx\n",
      "üìä Nodes: 63 (reduced from 180)\n",
      "üíæ Size: 2.62 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Verify Optimized Model (Updated)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: MODEL VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use the fixed model if it exists\n",
    "fixed_model_path = \"finn_build/ellipse_qonnx_streamlined_fixed.onnx\"\n",
    "if os.path.exists(fixed_model_path):\n",
    "    final_model_path = fixed_model_path\n",
    "    print(\"‚úì Using fixed streamlined model\")\n",
    "else:\n",
    "    final_model_path = streamlined_path if FINN_AVAILABLE else cleaned_path\n",
    "    print(\"‚ö†Ô∏è  Using original streamlined model\")\n",
    "\n",
    "optimized_model = onnx.load(final_model_path)\n",
    "\n",
    "print(\"\\n[1/3] Checking ONNX validity...\")\n",
    "onnx.checker.check_model(optimized_model)\n",
    "print(\"  ‚úì Model is valid ONNX\")\n",
    "\n",
    "print(\"\\n[2/3] Testing inference with ONNX Runtime...\")\n",
    "import onnxruntime as ort\n",
    "\n",
    "sess = ort.InferenceSession(final_model_path, providers=['CPUExecutionProvider'])\n",
    "\n",
    "# Get actual input/output names (they may have changed during streamlining)\n",
    "input_name = sess.get_inputs()[0].name\n",
    "output_name = sess.get_outputs()[0].name\n",
    "print(f\"  Model input: '{input_name}'\")\n",
    "print(f\"  Model output: '{output_name}'\")\n",
    "\n",
    "# Create dummy input\n",
    "dummy_input = np.random.randn(1, 1, 20, 20).astype(np.float32)\n",
    "\n",
    "# Run inference with correct input name\n",
    "output = sess.run(None, {input_name: dummy_input})\n",
    "print(f\"  ‚úì Inference successful\")\n",
    "print(f\"    Input shape: {dummy_input.shape}\")\n",
    "print(f\"    Output shape: {output[0].shape}\")\n",
    "print(f\"    Output range: [{output[0].min():.4f}, {output[0].max():.4f}]\")\n",
    "\n",
    "print(\"\\n[3/3] Model statistics...\")\n",
    "file_size_mb = os.path.getsize(final_model_path) / (1024*1024)\n",
    "print(f\"  File size: {file_size_mb:.2f} MB\")\n",
    "print(f\"  Total nodes: {len(optimized_model.graph.node)}\")\n",
    "\n",
    "# Count final node types\n",
    "final_node_types = Counter([n.op_type for n in optimized_model.graph.node])\n",
    "print(f\"\\n  Final node type distribution:\")\n",
    "for op_type, count in sorted(final_node_types.items(), key=lambda x: -x[1]):\n",
    "    print(f\"    {op_type}: {count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì OPTIMIZATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìÅ Optimized model: {final_model_path}\")\n",
    "print(f\"üìä Nodes: {len(optimized_model.graph.node)} (reduced from {sum(node_types.values())})\")\n",
    "print(f\"üíæ Size: {file_size_mb:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ellipse-qat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
