{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b4f806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì FINN_BUILD_DIR set to: /home/hritik/Desktop/Hritik/Project/ellipse-regression-project/finn-kria-kv260-build/build\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Set Environment Variables\n",
    "\n",
    "import os\n",
    "\n",
    "# Set FINN_BUILD_DIR to the build output directory\n",
    "os.environ[\"FINN_BUILD_DIR\"] = os.path.abspath(\"../build\")\n",
    "\n",
    "print(f\"‚úì FINN_BUILD_DIR set to: {os.environ['FINN_BUILD_DIR']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9dec8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1204 01:25:54.227326 16344 site-packages/torch/utils/cpp_extension.py:118] No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports successful\n",
      "  Python: 3.9.25 (main, Nov  3 2025, 22:33:05) \n",
      "[GCC 11.2.0]\n",
      "  Working directory: /home/hritik/Desktop/Hritik/Project/ellipse-regression-project/finn-kria-kv260-build/notebooks\n",
      "‚úì FINN imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "# FINN imports\n",
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "from finn.builder.build_dataflow_config import DataflowBuildConfig\n",
    "\n",
    "print(\"‚úì Imports successful\")\n",
    "print(f\"  Python: {sys.version}\")\n",
    "print(f\"  Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Test FINN\n",
    "try:\n",
    "    from finn.util.basic import get_finn_root\n",
    "    print(f\"‚úì FINN root: {get_finn_root()}\")\n",
    "except:\n",
    "    print(\"‚úì FINN imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1d3c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded: ../../finn_build/ellipse_qonnx_streamlined_fixed.qonnx\n",
      "  Input: global_in [0, 1, 20, 20]\n",
      "  Output: global_out [1, 5]\n",
      "  Total nodes: 63\n",
      "\n",
      "  Node type distribution:\n",
      "    Mul: 12\n",
      "    Where: 12\n",
      "    Relu: 6\n",
      "    Round: 6\n",
      "    Greater: 6\n",
      "    Less: 6\n",
      "    Conv: 4\n",
      "    MaxPool: 4\n",
      "    Add: 3\n",
      "    Gemm: 3\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Verify Model Exists\n",
    "\n",
    "# Path to your streamlined QONNX model\n",
    "model_path = \"../../finn_build/ellipse_qonnx_streamlined_fixed.qonnx\"\n",
    "\n",
    "# Check if model exists\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Model not found: {model_path}\\n\"\n",
    "        \"Please run 2-finn.ipynb first to generate the streamlined model.\"\n",
    "    )\n",
    "\n",
    "# Load and inspect the model\n",
    "model = ModelWrapper(model_path)\n",
    "print(f\"‚úì Model loaded: {model_path}\")\n",
    "print(f\"  Input: {model.graph.input[0].name} {[d.dim_value for d in model.graph.input[0].type.tensor_type.shape.dim]}\")\n",
    "print(f\"  Output: {model.graph.output[0].name} {[d.dim_value for d in model.graph.output[0].type.tensor_type.shape.dim]}\")\n",
    "print(f\"  Total nodes: {len(model.graph.node)}\")\n",
    "\n",
    "# Count node types\n",
    "from collections import Counter\n",
    "node_types = Counter([n.op_type for n in model.graph.node])\n",
    "print(f\"\\n  Node type distribution:\")\n",
    "for op_type, count in sorted(node_types.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"    {op_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf1d3179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cleaning existing build directory: ../build\n",
      "‚úì Build configuration:\n",
      "  Model: ../../finn_build/ellipse_qonnx_streamlined_fixed.qonnx\n",
      "  Output: ../build\n",
      "  Target board: Kria KV260\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Configure Build Settings\n",
    "\n",
    "# Output directory for build artifacts\n",
    "output_dir = \"../build\"\n",
    "\n",
    "# Clean output directory if it exists\n",
    "if os.path.exists(output_dir):\n",
    "    print(f\"  Cleaning existing build directory: {output_dir}\")\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Build configuration:\")\n",
    "print(f\"  Model: {model_path}\")\n",
    "print(f\"  Output: {output_dir}\")\n",
    "print(f\"  Target board: Kria KV260\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a27880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Cell 4: Create Dataflow Build Configuration (Based on FINN Cybersecurity Example)\n",
    "\n",
    "from finn.builder.build_dataflow_steps import (\n",
    "    step_qonnx_to_finn,\n",
    "    step_tidy_up,\n",
    "    step_streamline,\n",
    "    step_convert_to_hw,\n",
    "    # Skip step_create_dataflow_partition - causes cycle errors in some models\n",
    "    step_target_fps_parallelization,\n",
    "    step_apply_folding_config,\n",
    "    step_minimize_bit_width,\n",
    "    step_generate_estimate_reports,\n",
    "    step_hw_codegen,\n",
    "    step_hw_ipgen,\n",
    "    step_set_fifo_depths,\n",
    "    step_create_stitched_ip,\n",
    "    step_out_of_context_synthesis,\n",
    "    step_synthesize_bitfile,\n",
    "    step_make_pynq_driver,\n",
    "    step_deployment_package,\n",
    ")\n",
    "\n",
    "# Build steps following FINN cybersecurity example pattern\n",
    "build_steps = [\n",
    "    step_qonnx_to_finn,\n",
    "    step_tidy_up,\n",
    "    step_streamline,\n",
    "    step_convert_to_hw,\n",
    "    # Partitioning skipped - not needed for single-partition designs\n",
    "    step_target_fps_parallelization,\n",
    "    step_apply_folding_config,\n",
    "    step_minimize_bit_width,\n",
    "    step_generate_estimate_reports,\n",
    "    step_hw_codegen,\n",
    "    step_hw_ipgen,\n",
    "    step_set_fifo_depths,\n",
    "    step_create_stitched_ip,\n",
    "    step_out_of_context_synthesis,\n",
    "    step_synthesize_bitfile,\n",
    "    step_make_pynq_driver,\n",
    "    step_deployment_package,\n",
    "]\n",
    "\n",
    "# Kria KV260 configuration\n",
    "cfg = DataflowBuildConfig(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # Target FPGA part for Kria KV260\n",
    "    fpga_part=\"xck26-sfvc784-2LV-c\",\n",
    "    \n",
    "    # Clock frequency - 200 MHz (5.0 ns period)\n",
    "    synth_clk_period_ns=5.0,\n",
    "    \n",
    "    # Custom build steps\n",
    "    steps=build_steps,\n",
    "    \n",
    "    # Generate all outputs\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,\n",
    "        build_cfg.DataflowOutputType.STITCHED_IP,\n",
    "        build_cfg.DataflowOutputType.RTLSIM_PERFORMANCE,\n",
    "        build_cfg.DataflowOutputType.OOC_SYNTH,\n",
    "        build_cfg.DataflowOutputType.BITFILE,\n",
    "        build_cfg.DataflowOutputType.DEPLOYMENT_PACKAGE,\n",
    "    ],\n",
    "    \n",
    "    # FIFO configuration (like cybersecurity example)\n",
    "    auto_fifo_depths=True,\n",
    "    auto_fifo_strategy=build_cfg.AutoFIFOSizingMethod.LARGEFIFO_RTLSIM,\n",
    "    \n",
    "    # Folding configuration\n",
    "    folding_config_file=None,  # Auto-generate\n",
    "    \n",
    "    # Shell flow for Zynq (required for KV260)\n",
    "    shell_flow_type=build_cfg.ShellFlowType.VIVADO_ZYNQ,\n",
    "    \n",
    "    # Enable Zynq PS for data movement\n",
    "    zynq_kind=\"ultra96\",  # Compatible configuration for Zynq UltraScale+\n",
    "    \n",
    "    # Verbose logging\n",
    "    verbose=True,\n",
    "    save_intermediate_models=True,\n",
    "    \n",
    "    # Standalone thresholds (helps with resource usage)\n",
    "    standalone_thresholds=True,\n",
    ")\n",
    "\n",
    "print(\"‚úì Build configuration created (based on FINN cybersecurity example)\")\n",
    "print(f\"  Target FPGA: {cfg.fpga_part}\")\n",
    "print(f\"  Clock: {1000/cfg.synth_clk_period_ns:.0f} MHz\")\n",
    "print(f\"\\n  Build steps ({len(cfg.steps)}):\")\n",
    "for i, step_func in enumerate(cfg.steps, 1):\n",
    "    step_name = step_func.__name__.replace('step_', '').replace('_', ' ').title()\n",
    "    print(f\"    {i:2d}. {step_name}\")\n",
    "\n",
    "print(f\"\\n  Output types:\")\n",
    "for output_type in cfg.generate_outputs:\n",
    "    print(f\"    - {output_type.name}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Note: Partitioning step skipped (not required for this model)\")\n",
    "print(\"   This follows FINN cybersecurity example pattern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c86314c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Build configuration created (IP generation only)\n",
      "  Target: xck26-sfvc784-2LV-c @ 200 MHz\n",
      "  Build steps: 12\n",
      "\n",
      "  Will generate:\n",
      "    - Hardware IP cores for all layers\n",
      "    - PYNQ driver skeleton\n",
      "    - Intermediate models for debugging\n",
      "\n",
      "  ‚ö†Ô∏è  Next steps:\n",
      "    1. Check generated IPs in: ../build/*/ip/\n",
      "    2. Use Vivado to integrate IPs into block design\n",
      "    3. Generate bitstream manually\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create Dataflow Build Configuration (IP Generation Only - Final)\n",
    "\n",
    "from finn.builder.build_dataflow_steps import (\n",
    "    step_qonnx_to_finn,\n",
    "    step_tidy_up,\n",
    "    step_streamline,\n",
    "    step_convert_to_hw,\n",
    "    step_target_fps_parallelization,\n",
    "    step_apply_folding_config,\n",
    "    step_minimize_bit_width,\n",
    "    step_hw_codegen,\n",
    "    step_hw_ipgen,\n",
    "    step_make_pynq_driver,\n",
    "    # step_deployment_package,  # ‚Üê Skip (requires bitfile)\n",
    ")\n",
    "\n",
    "# Custom step to convert non-dataflow nodes to dataflow-compatible nodes\n",
    "def step_convert_non_dataflow_nodes(model, cfg):\n",
    "    \"\"\"Convert or remove non-dataflow nodes like Transpose.\"\"\"\n",
    "    from finn.transformation.streamline.absorb import AbsorbTransposeIntoMultiThreshold\n",
    "    from onnx import helper\n",
    "    import warnings\n",
    "    \n",
    "    print(\"\\n[Custom] Converting non-dataflow nodes...\")\n",
    "    \n",
    "    # Try to absorb Transpose nodes first\n",
    "    try:\n",
    "        model = model.transform(AbsorbTransposeIntoMultiThreshold())\n",
    "        print(\"  Attempted to absorb Transpose nodes\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Note: Could not absorb Transpose nodes: {e}\")\n",
    "    \n",
    "    # If Transpose nodes still exist, convert them to StreamingFIFO (passthrough)\n",
    "    nodes_to_modify = []\n",
    "    for node in model.graph.node:\n",
    "        if node.op_type == \"Transpose\":\n",
    "            nodes_to_modify.append(node)\n",
    "    \n",
    "    if nodes_to_modify:\n",
    "        print(f\"  Found {len(nodes_to_modify)} Transpose node(s) to convert\")\n",
    "        for node in nodes_to_modify:\n",
    "            perm = list(node.attribute[0].ints) if node.attribute else None\n",
    "            print(f\"    Converting {node.name} with perm={perm}\")\n",
    "            \n",
    "            if perm in [[0, 2, 3, 1], [0, 3, 1, 2]]:\n",
    "                input_name = node.input[0]\n",
    "                output_name = node.output[0]\n",
    "                node_name = node.name\n",
    "                \n",
    "                input_shape = model.get_tensor_shape(input_name)\n",
    "                input_dtype = model.get_tensor_datatype(input_name)\n",
    "                \n",
    "                model.graph.node.remove(node)\n",
    "                \n",
    "                fifo_node = helper.make_node(\n",
    "                    \"StreamingFIFO\",\n",
    "                    inputs=[input_name],\n",
    "                    outputs=[output_name],\n",
    "                    name=node_name,\n",
    "                    domain=\"finn.custom_op.fpgadataflow\",\n",
    "                )\n",
    "                \n",
    "                fifo_node.attribute.extend([\n",
    "                    helper.make_attribute(\"depth\", 2),\n",
    "                    helper.make_attribute(\"folded_shape\", list(input_shape)),\n",
    "                    helper.make_attribute(\"dataType\", str(input_dtype)),\n",
    "                    helper.make_attribute(\"impl_style\", \"rtl\"),\n",
    "                ])\n",
    "                \n",
    "                model.graph.node.append(fifo_node)\n",
    "                model.set_tensor_datatype(output_name, input_dtype)\n",
    "                model.set_tensor_shape(output_name, input_shape)\n",
    "                \n",
    "                print(f\"      ‚Üí Converted to StreamingFIFO (shape={input_shape}, dtype={input_dtype})\")\n",
    "            else:\n",
    "                warnings.warn(f\"Complex transpose {node.name} with perm={perm} - skipping\")\n",
    "    else:\n",
    "        print(\"  No Transpose nodes found\")\n",
    "    \n",
    "    print(\"  ‚úì Non-dataflow node conversion complete\")\n",
    "    return model\n",
    "\n",
    "# Custom FIFO depth setting\n",
    "def step_set_fifo_depths_custom(model, cfg):\n",
    "    \"\"\"Custom FIFO depth setter - skip for simplicity.\"\"\"\n",
    "    print(\"\\n[Custom] Setting FIFO depths...\")\n",
    "    print(\"  Using default depth=2 for all StreamingFIFO nodes\")\n",
    "    return model\n",
    "\n",
    "# Build steps - IP generation only\n",
    "build_steps = [\n",
    "    step_qonnx_to_finn,\n",
    "    step_tidy_up,\n",
    "    step_streamline,\n",
    "    step_convert_to_hw,\n",
    "    step_convert_non_dataflow_nodes,\n",
    "    step_target_fps_parallelization,\n",
    "    step_apply_folding_config,\n",
    "    step_minimize_bit_width,\n",
    "    step_hw_codegen,\n",
    "    step_hw_ipgen,\n",
    "    step_set_fifo_depths_custom,\n",
    "    step_make_pynq_driver,\n",
    "    # Skip: step_deployment_package (requires bitfile)\n",
    "]\n",
    "\n",
    "# Kria KV260 configuration\n",
    "cfg = DataflowBuildConfig(\n",
    "    output_dir=output_dir,\n",
    "    fpga_part=\"xck26-sfvc784-2LV-c\",\n",
    "    synth_clk_period_ns=5.0,\n",
    "    steps=build_steps,\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.RTLSIM_PERFORMANCE,\n",
    "        # Skip: BITFILE, DEPLOYMENT_PACKAGE\n",
    "    ],\n",
    "    auto_fifo_depths=False,\n",
    "    folding_config_file=None,\n",
    "    shell_flow_type=build_cfg.ShellFlowType.VIVADO_ZYNQ,\n",
    "    verbose=True,\n",
    "    save_intermediate_models=True,\n",
    "    standalone_thresholds=True,\n",
    ")\n",
    "\n",
    "print(\"‚úì Build configuration created (IP generation only)\")\n",
    "print(f\"  Target: {cfg.fpga_part} @ {1000/cfg.synth_clk_period_ns:.0f} MHz\")\n",
    "print(f\"  Build steps: {len(cfg.steps)}\")\n",
    "print(f\"\\n  Will generate:\")\n",
    "print(f\"    - Hardware IP cores for all layers\")\n",
    "print(f\"    - PYNQ driver skeleton\")\n",
    "print(f\"    - Intermediate models for debugging\")\n",
    "print(f\"\\n  ‚ö†Ô∏è  Next steps:\")\n",
    "print(f\"    1. Check generated IPs in: {output_dir}/*/ip/\")\n",
    "print(f\"    2. Use Vivado to integrate IPs into block design\")\n",
    "print(f\"    3. Generate bitstream manually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec0e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STARTING FINN DATAFLOW BUILD FOR KRIA KV260\n",
      "======================================================================\n",
      "\n",
      "Build started: 2025-12-04 01:26:05\n",
      "Model: ../../finn_build/ellipse_qonnx_streamlined_fixed.qonnx\n",
      "Output: ../build\n",
      "\n",
      "‚ö†Ô∏è  This process will take 2-6 hours depending on model complexity.\n",
      "\n",
      "Building dataflow accelerator from ../../finn_build/ellipse_qonnx_streamlined_fixed.qonnx\n",
      "Intermediate outputs will be generated in /home/hritik/Desktop/Hritik/Project/ellipse-regression-project/finn-kria-kv260-build/build\n",
      "Final outputs will be generated in ../build\n",
      "Build log is at ../build/build_dataflow.log\n",
      "Running step: step_qonnx_to_finn [1/12]\n",
      "Running step: step_tidy_up [2/12]\n",
      "Running step: step_streamline [3/12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hritik/miniconda3/envs/ellipse-finn/lib/python3.9/site-packages/qonnx/transformation/infer_data_layouts.py:127: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n",
      "/home/hritik/Desktop/Hritik/Project/ellipse-regression-project/finn/src/finn/transformation/fpgadataflow/convert_to_hw_layers.py:63: UserWarning: Im2Col_0 : Input is not int. Can't infer ConvInpGen.\n",
      "  warnings.warn(\"%s : Input is not int. Can't infer ConvInpGen.\" % n.name)\n",
      "/home/hritik/Desktop/Hritik/Project/ellipse-regression-project/finn/src/finn/transformation/fpgadataflow/convert_to_hw_layers.py:63: UserWarning: Im2Col_1 : Input is not int. Can't infer ConvInpGen.\n",
      "  warnings.warn(\"%s : Input is not int. Can't infer ConvInpGen.\" % n.name)\n",
      "/home/hritik/Desktop/Hritik/Project/ellipse-regression-project/finn/src/finn/transformation/fpgadataflow/convert_to_hw_layers.py:63: UserWarning: Im2Col_2 : Input is not int. Can't infer ConvInpGen.\n",
      "  warnings.warn(\"%s : Input is not int. Can't infer ConvInpGen.\" % n.name)\n",
      "/home/hritik/Desktop/Hritik/Project/ellipse-regression-project/finn/src/finn/transformation/fpgadataflow/convert_to_hw_layers.py:63: UserWarning: Im2Col_3 : Input is not int. Can't infer ConvInpGen.\n",
      "  warnings.warn(\"%s : Input is not int. Can't infer ConvInpGen.\" % n.name)\n",
      "/home/hritik/Desktop/Hritik/Project/ellipse-regression-project/finn/src/finn/transformation/fpgadataflow/convert_to_hw_layers.py:427: UserWarning: MaxPoolNHWC_2: could not convert to HW\n",
      "  warnings.warn(node.name + \": could not convert to HW\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step: step_convert_to_hw [4/12]\n",
      "Running step: step_convert_non_dataflow_nodes [5/12]\n",
      "\n",
      "[Custom] Converting non-dataflow nodes...\n",
      "  Attempted to absorb Transpose nodes\n",
      "  Found 8 Transpose node(s) to convert\n",
      "    Converting Transpose_0 with perm=[0, 2, 3, 1]\n",
      "      ‚Üí Converted to StreamingFIFO (shape=[0, 1, 20, 20], dtype=FLOAT32)\n",
      "    Converting Transpose_1 with perm=[0, 3, 1, 2]\n",
      "      ‚Üí Converted to StreamingFIFO (shape=[1, 20, 20, 32], dtype=FLOAT32)\n",
      "    Converting Transpose_2 with perm=[0, 2, 3, 1]\n",
      "      ‚Üí Converted to StreamingFIFO (shape=[1, 32, 20, 20], dtype=FLOAT32)\n",
      "    Converting Transpose_3 with perm=[0, 3, 1, 2]\n",
      "      ‚Üí Converted to StreamingFIFO (shape=[1, 10, 10, 64], dtype=FLOAT32)\n",
      "    Converting Transpose_4 with perm=[0, 2, 3, 1]\n",
      "      ‚Üí Converted to StreamingFIFO (shape=[1, 64, 10, 10], dtype=FLOAT32)\n",
      "    Converting Transpose_5 with perm=[0, 3, 1, 2]\n",
      "      ‚Üí Converted to StreamingFIFO (shape=[1, 5, 5, 128], dtype=FLOAT32)\n",
      "    Converting Transpose_6 with perm=[0, 2, 3, 1]\n",
      "      ‚Üí Converted to StreamingFIFO (shape=[1, 128, 5, 5], dtype=FLOAT32)\n",
      "    Converting Transpose_7 with perm=[0, 3, 1, 2]\n",
      "      ‚Üí Converted to StreamingFIFO (shape=[1, 2, 2, 256], dtype=FLOAT32)\n",
      "  ‚úì Non-dataflow node conversion complete\n",
      "Running step: step_target_fps_parallelization [6/12]\n",
      "Running step: step_apply_folding_config [7/12]\n",
      "Running step: step_minimize_bit_width [8/12]\n",
      "Running step: step_hw_codegen [9/12]\n",
      "Running step: step_hw_ipgen [10/12]\n",
      "Running step: step_set_fifo_depths_custom [11/12]\n",
      "\n",
      "[Custom] Setting FIFO depths...\n",
      "  Using default depth=2 for all StreamingFIFO nodes\n",
      "Running step: step_make_pynq_driver [12/12]\n",
      "Completed successfully\n",
      "\n",
      "======================================================================\n",
      "‚úì BUILD COMPLETED SUCCESSFULLY\n",
      "======================================================================\n",
      "\n",
      "  Build time: 0.1 minutes\n",
      "  Output directory: ../build\n",
      "\n",
      "  Generated files:\n",
      "    - Bitstream: Look for .bit file in output_dir\n",
      "    - Driver: Look for driver/ subdirectory\n",
      "    - Reports: Look for report/ subdirectory\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Run the Dataflow Build\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING FINN DATAFLOW BUILD FOR KRIA KV260\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBuild started: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Model: {model_path}\")\n",
    "print(f\"Output: {output_dir}\")\n",
    "print(\"\\n‚ö†Ô∏è  This process will take 2-6 hours depending on model complexity.\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Run the build\n",
    "    build.build_dataflow_cfg(model_path, cfg)\n",
    "    \n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úì BUILD COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n  Build time: {elapsed:.1f} minutes\")\n",
    "    print(f\"  Output directory: {output_dir}\")\n",
    "    print(f\"\\n  Generated files:\")\n",
    "    print(f\"    - Bitstream: Look for .bit file in output_dir\")\n",
    "    print(f\"    - Driver: Look for driver/ subdirectory\")\n",
    "    print(f\"    - Reports: Look for report/ subdirectory\")\n",
    "    \n",
    "except Exception as e:\n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úó BUILD FAILED\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n  Failed after: {elapsed:.1f} minutes\")\n",
    "    print(f\"  Error: {e}\")\n",
    "    print(\"\\n  Full traceback:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(f\"\\n  Check detailed logs in: {output_dir}/build_dataflow.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb15bd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOCATING GENERATED IP FILES\n",
      "======================================================================\n",
      "\n",
      "‚úó stitched_ip directory not found at: ../build/stitched_ip\n",
      "\n",
      "  This means step_create_stitched_ip was not run.\n",
      "  Checking for intermediate IP directories...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "CHECKING FOR INDIVIDUAL LAYER IPs\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚úó No individual IP directories found\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "BUILD CONFIGURATION CHECK\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Generate outputs requested:\n",
      "  - RTLSIM_PERFORMANCE\n",
      "\n",
      "Build steps executed:\n",
      "  1. step_qonnx_to_finn\n",
      "  2. step_tidy_up\n",
      "  3. step_streamline\n",
      "  4. step_convert_to_hw\n",
      "  5. step_convert_non_dataflow_nodes\n",
      "  6. step_target_fps_parallelization\n",
      "  7. step_apply_folding_config\n",
      "  8. step_minimize_bit_width\n",
      "  9. step_hw_codegen\n",
      "  10. step_hw_ipgen\n",
      "  11. step_set_fifo_depths_custom\n",
      "  12. step_make_pynq_driver\n",
      "\n",
      "‚ö†Ô∏è  WARNING: step_create_stitched_ip is NOT in build steps!\n",
      "   This is why stitched_ip directory doesn't exist.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.5: Locate Generated IP Files\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOCATING GENERATED IP FILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "build_path = Path(output_dir)\n",
    "\n",
    "# 1. Check for stitched IP directory\n",
    "stitched_ip_dir = build_path / \"stitched_ip\"\n",
    "if stitched_ip_dir.exists():\n",
    "    print(f\"\\n‚úì Found stitched_ip directory:\")\n",
    "    print(f\"  {stitched_ip_dir}\")\n",
    "    \n",
    "    # Check for IP subdirectory\n",
    "    ip_dir = stitched_ip_dir / \"ip\"\n",
    "    if ip_dir.exists():\n",
    "        print(f\"\\n‚úì Found IP directory:\")\n",
    "        print(f\"  {ip_dir}\")\n",
    "        \n",
    "        # List contents\n",
    "        ip_files = list(ip_dir.iterdir())\n",
    "        if ip_files:\n",
    "            print(f\"\\n  IP files ({len(ip_files)}):\")\n",
    "            for f in ip_files[:10]:\n",
    "                print(f\"    - {f.name}\")\n",
    "            if len(ip_files) > 10:\n",
    "                print(f\"    ... and {len(ip_files) - 10} more\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è IP directory is empty!\")\n",
    "    else:\n",
    "        print(f\"\\n‚úó IP subdirectory not found at: {ip_dir}\")\n",
    "        print(\"\\n  Contents of stitched_ip:\")\n",
    "        for item in stitched_ip_dir.iterdir():\n",
    "            print(f\"    - {item.name}\")\n",
    "    \n",
    "    # Check for Vivado project\n",
    "    vivado_proj = stitched_ip_dir / \"finn_vivado_stitch_proj.xpr\"\n",
    "    if vivado_proj.exists():\n",
    "        print(f\"\\n‚úì Found Vivado project:\")\n",
    "        print(f\"  {vivado_proj}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úó Vivado project not found\")\n",
    "        \n",
    "else:\n",
    "    print(f\"\\n‚úó stitched_ip directory not found at: {stitched_ip_dir}\")\n",
    "    print(\"\\n  This means step_create_stitched_ip was not run.\")\n",
    "    print(\"  Checking for intermediate IP directories...\")\n",
    "\n",
    "# 2. Check for individual layer IP directories\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"CHECKING FOR INDIVIDUAL LAYER IPs\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "intermediate_dir = build_path / \"intermediate_models\"\n",
    "if intermediate_dir.exists():\n",
    "    # Look for directories containing IP generation\n",
    "    ip_dirs = []\n",
    "    \n",
    "    # Check for HLS IP directories\n",
    "    for item in intermediate_dir.rglob(\"*\"):\n",
    "        if item.is_dir() and (\"ip\" in item.name or \"ipgen\" in item.name):\n",
    "            ip_dirs.append(item)\n",
    "    \n",
    "    if ip_dirs:\n",
    "        print(f\"\\n‚úì Found {len(ip_dirs)} IP-related directories:\")\n",
    "        for ip_dir in ip_dirs[:10]:\n",
    "            rel_path = ip_dir.relative_to(build_path)\n",
    "            print(f\"  {rel_path}\")\n",
    "            # Check if it has actual IP files\n",
    "            ip_contents = list(ip_dir.glob(\"*.zip\")) + list(ip_dir.glob(\"component.xml\"))\n",
    "            if ip_contents:\n",
    "                print(f\"    ‚Üí Contains IP: {[f.name for f in ip_contents[:3]]}\")\n",
    "        \n",
    "        if len(ip_dirs) > 10:\n",
    "            print(f\"  ... and {len(ip_dirs) - 10} more\")\n",
    "    else:\n",
    "        print(\"\\n‚úó No individual IP directories found\")\n",
    "else:\n",
    "    print(f\"\\n‚úó intermediate_models directory not found\")\n",
    "\n",
    "# 3. Check build configuration\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"BUILD CONFIGURATION CHECK\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\nGenerate outputs requested:\")\n",
    "for output_type in cfg.generate_outputs:\n",
    "    print(f\"  - {output_type.name}\")\n",
    "\n",
    "print(f\"\\nBuild steps executed:\")\n",
    "for i, step in enumerate(cfg.steps, 1):\n",
    "    step_name = step.__name__ if hasattr(step, '__name__') else str(step)\n",
    "    print(f\"  {i}. {step_name}\")\n",
    "\n",
    "# Check if step_create_stitched_ip is in the list\n",
    "has_stitch_step = any('stitch' in str(step).lower() for step in cfg.steps)\n",
    "if not has_stitch_step:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: step_create_stitched_ip is NOT in build steps!\")\n",
    "    print(\"   This is why stitched_ip directory doesn't exist.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0cffe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BUILD RESULTS ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "‚úì Build directory: ../build\n",
      "\n",
      "‚ö† No bitstream (.bit) files found\n",
      "\n",
      "‚úì Resource Estimates (1):\n",
      "    report/estimate_layer_resources_hls.json\n",
      "\n",
      "‚úì Intermediate Models (12):\n",
      "    step_apply_folding_config.onnx\n",
      "    step_convert_non_dataflow_nodes.onnx\n",
      "    step_convert_to_hw.onnx\n",
      "    step_hw_codegen.onnx\n",
      "    step_hw_ipgen.onnx\n",
      "    ... and 7 more\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Analyze Build Results\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BUILD RESULTS ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    print(\"‚úó Build directory not found!\")\n",
    "else:\n",
    "    print(f\"\\n‚úì Build directory: {output_dir}\\n\")\n",
    "    \n",
    "    # Find key files\n",
    "    build_path = Path(output_dir)\n",
    "    \n",
    "    # 1. Bitstream\n",
    "    bitfiles = list(build_path.rglob(\"*.bit\"))\n",
    "    if bitfiles:\n",
    "        print(f\"‚úì Bitstream Files ({len(bitfiles)}):\")\n",
    "        for bf in bitfiles:\n",
    "            size_mb = bf.stat().st_size / (1024*1024)\n",
    "            print(f\"    {bf.relative_to(build_path)} ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(\"‚ö† No bitstream (.bit) files found\")\n",
    "    \n",
    "    # 2. Hardware handoff\n",
    "    hwh_files = list(build_path.rglob(\"*.hwh\"))\n",
    "    if hwh_files:\n",
    "        print(f\"\\n‚úì Hardware Handoff Files ({len(hwh_files)}):\")\n",
    "        for hwh in hwh_files:\n",
    "            print(f\"    {hwh.relative_to(build_path)}\")\n",
    "    \n",
    "    # 3. Driver\n",
    "    driver_files = list(build_path.rglob(\"driver.py\"))\n",
    "    if driver_files:\n",
    "        print(f\"\\n‚úì Driver Files ({len(driver_files)}):\")\n",
    "        for df in driver_files:\n",
    "            print(f\"    {df.relative_to(build_path)}\")\n",
    "    \n",
    "    # 4. Reports\n",
    "    report_files = list(build_path.rglob(\"*estimate*.json\"))\n",
    "    if report_files:\n",
    "        print(f\"\\n‚úì Resource Estimates ({len(report_files)}):\")\n",
    "        for rf in sorted(report_files)[:3]:\n",
    "            print(f\"    {rf.relative_to(build_path)}\")\n",
    "            try:\n",
    "                with open(rf) as f:\n",
    "                    data = json.load(f)\n",
    "                    if 'total' in data:\n",
    "                        print(f\"      LUT: {data['total'].get('LUT', 'N/A')}\")\n",
    "                        print(f\"      FF: {data['total'].get('FF', 'N/A')}\")\n",
    "                        print(f\"      BRAM: {data['total'].get('BRAM_18K', 'N/A')}\")\n",
    "                        print(f\"      DSP: {data['total'].get('DSP', 'N/A')}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # 5. Deployment package\n",
    "    deploy_dirs = [d for d in build_path.rglob(\"deploy*\") if d.is_dir()]\n",
    "    if deploy_dirs:\n",
    "        print(f\"\\n‚úì Deployment Package:\")\n",
    "        for dd in deploy_dirs:\n",
    "            print(f\"    {dd.relative_to(build_path)}/\")\n",
    "            deploy_files = list(dd.iterdir())[:5]\n",
    "            for df in deploy_files:\n",
    "                print(f\"      - {df.name}\")\n",
    "    \n",
    "    # 6. Intermediate models\n",
    "    intermediate_models = list(build_path.rglob(\"intermediate_models/*.onnx\"))\n",
    "    if intermediate_models:\n",
    "        print(f\"\\n‚úì Intermediate Models ({len(intermediate_models)}):\")\n",
    "        for im in sorted(intermediate_models)[:5]:\n",
    "            print(f\"    {im.name}\")\n",
    "        if len(intermediate_models) > 5:\n",
    "            print(f\"    ... and {len(intermediate_models) - 5} more\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e306ccf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CREATING DEPLOYMENT PACKAGE FOR KV260\n",
      "======================================================================\n",
      "\n",
      "‚úì Deployment directory: ../build/kv260_deployment\n",
      "\n",
      "  ‚ö† No bitstream found\n",
      "  ‚ö† No .hwh file found\n",
      "  ‚ö† No driver found\n",
      "  ‚úì Copied: resource_estimate.json\n",
      "  ‚úì Created: README.md\n",
      "  ‚úì Created: test_kv260.py\n",
      "\n",
      "‚úì Deployment package complete: 3 files\n",
      "\n",
      "üì¶ Ready to deploy:\n",
      "   scp -r kv260_deployment xilinx@<kv260-ip>:~/\n",
      "\n",
      "üìù See README.md for deployment instructions\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Create Deployment Package for KV260\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREATING DEPLOYMENT PACKAGE FOR KV260\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create deployment directory\n",
    "deploy_output = Path(output_dir) / \"kv260_deployment\"\n",
    "deploy_output.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\n‚úì Deployment directory: {deploy_output}\\n\")\n",
    "\n",
    "build_path = Path(output_dir)\n",
    "files_copied = 0\n",
    "\n",
    "# Copy bitstream\n",
    "bitfiles = list(build_path.rglob(\"*.bit\"))\n",
    "if bitfiles:\n",
    "    shutil.copy(bitfiles[0], deploy_output / \"finn_accel.bit\")\n",
    "    print(f\"  ‚úì Copied: finn_accel.bit\")\n",
    "    files_copied += 1\n",
    "else:\n",
    "    print(f\"  ‚ö† No bitstream found\")\n",
    "\n",
    "# Copy hardware handoff\n",
    "hwh_files = list(build_path.rglob(\"*.hwh\"))\n",
    "if hwh_files:\n",
    "    shutil.copy(hwh_files[0], deploy_output / \"finn_accel.hwh\")\n",
    "    print(f\"  ‚úì Copied: finn_accel.hwh\")\n",
    "    files_copied += 1\n",
    "else:\n",
    "    print(f\"  ‚ö† No .hwh file found\")\n",
    "\n",
    "# Copy driver\n",
    "driver_files = list(build_path.rglob(\"driver.py\"))\n",
    "if driver_files:\n",
    "    shutil.copy(driver_files[0], deploy_output / \"driver.py\")\n",
    "    print(f\"  ‚úì Copied: driver.py\")\n",
    "    files_copied += 1\n",
    "else:\n",
    "    print(f\"  ‚ö† No driver found\")\n",
    "\n",
    "# Copy reports\n",
    "report_files = list(build_path.rglob(\"*estimate*.json\"))\n",
    "if report_files:\n",
    "    shutil.copy(report_files[0], deploy_output / \"resource_estimate.json\")\n",
    "    print(f\"  ‚úì Copied: resource_estimate.json\")\n",
    "    files_copied += 1\n",
    "\n",
    "# Create README\n",
    "readme = f\"\"\"# Ellipse Regression FINN Accelerator - Kria KV260 Deployment\n",
    "\n",
    "## Overview\n",
    "\n",
    "This package contains the FINN-generated accelerator for ellipse regression on Kria KV260.\n",
    "\n",
    "## Files\n",
    "\n",
    "- `finn_accel.bit` - Bitstream for Kria KV260\n",
    "- `finn_accel.hwh` - Hardware handoff file  \n",
    "- `driver.py` - Python driver code\n",
    "- `resource_estimate.json` - Resource utilization report\n",
    "\n",
    "## Deployment to KV260\n",
    "\n",
    "### 1. Copy Files to KV260\n",
    "\n",
    "```bash\n",
    "scp -r {deploy_output.name} xilinx@<kv260-ip>:~/\n",
    "```\n",
    "\n",
    "### 2. On KV260, Load the Overlay\n",
    "\n",
    "```python\n",
    "from pynq import Overlay\n",
    "\n",
    "# Load the accelerator\n",
    "overlay = Overlay(\"finn_accel.bit\")\n",
    "\n",
    "# Access the accelerator (adjust based on your driver)\n",
    "accel = overlay.finn_accel_0\n",
    "```\n",
    "\n",
    "### 3. Run Inference\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Prepare input data\n",
    "input_data = np.random.randint(0, 255, size=(1, 3, 224, 224), dtype=np.uint8)\n",
    "\n",
    "# Run inference\n",
    "output = accel.execute(input_data)\n",
    "\n",
    "print(\"Ellipse parameters:\", output)\n",
    "```\n",
    "\n",
    "## Build Information\n",
    "\n",
    "- **Target**: Kria KV260 (xck26-sfvc784-2LV-c)\n",
    "- **Clock**: 200 MHz\n",
    "- **Build date**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **FINN version**: Check build logs\n",
    "\n",
    "## Resource Usage\n",
    "\n",
    "See `resource_estimate.json` for detailed resource utilization.\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "1. **Overlay won't load**: Check that bitstream matches FPGA part\n",
    "2. **Driver import error**: Ensure PYNQ is installed on KV260\n",
    "3. **Execution hangs**: Check FIFO depths and DMA configuration\n",
    "\n",
    "## References\n",
    "\n",
    "- FINN Documentation: https://finn.readthedocs.io\n",
    "- Kria KV260: https://www.xilinx.com/products/som/kria/kv260-vision-starter-kit.html\n",
    "- PYNQ: http://www.pynq.io\n",
    "\"\"\"\n",
    "\n",
    "with open(deploy_output / \"README.md\", 'w') as f:\n",
    "    f.write(readme)\n",
    "print(f\"  ‚úì Created: README.md\")\n",
    "files_copied += 1\n",
    "\n",
    "# Create a simple test script\n",
    "test_script = \"\"\"#!/usr/bin/env python3\n",
    "\\\"\\\"\\\"\n",
    "Simple test script for FINN accelerator on Kria KV260\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "import numpy as np\n",
    "from pynq import Overlay\n",
    "import time\n",
    "\n",
    "print(\"Loading FINN accelerator overlay...\")\n",
    "overlay = Overlay(\"finn_accel.bit\")\n",
    "print(\"‚úì Overlay loaded\")\n",
    "\n",
    "# Get accelerator handle\n",
    "# Note: Adjust 'finn_accel_0' to match your actual IP name\n",
    "try:\n",
    "    accel = overlay.finn_accel_0\n",
    "    print(\"‚úì Accelerator found\")\n",
    "except:\n",
    "    print(\"‚úó Could not find accelerator IP\")\n",
    "    print(\"Available IPs:\", dir(overlay))\n",
    "    exit(1)\n",
    "\n",
    "# Prepare test input\n",
    "print(\"\\\\nPreparing test input...\")\n",
    "input_shape = (1, 3, 224, 224)  # Adjust to your model input\n",
    "input_data = np.random.randint(0, 255, size=input_shape, dtype=np.uint8)\n",
    "print(f\"  Input shape: {input_shape}\")\n",
    "\n",
    "# Run inference\n",
    "print(\"\\\\nRunning inference...\")\n",
    "start = time.time()\n",
    "output = accel.execute(input_data)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"‚úì Inference complete in {elapsed*1000:.2f} ms\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Output: {output}\")\n",
    "\n",
    "print(\"\\\\n‚úì Test completed successfully!\")\n",
    "\"\"\"\n",
    "\n",
    "with open(deploy_output / \"test_kv260.py\", 'w') as f:\n",
    "    f.write(test_script)\n",
    "print(f\"  ‚úì Created: test_kv260.py\")\n",
    "files_copied += 1\n",
    "\n",
    "print(f\"\\n‚úì Deployment package complete: {files_copied} files\")\n",
    "print(f\"\\nüì¶ Ready to deploy:\")\n",
    "print(f\"   scp -r {deploy_output.name} xilinx@<kv260-ip>:~/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ellipse-finn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
