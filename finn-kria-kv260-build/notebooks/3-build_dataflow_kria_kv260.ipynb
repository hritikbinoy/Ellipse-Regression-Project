{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b4f806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ FINN_BUILD_DIR set to: /home/hritik/Desktop/Hritik/Project/ellipse-regression-project/finn-kria-kv260-build/build\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Set Environment Variables\n",
    "\n",
    "import os\n",
    "\n",
    "# Set FINN_BUILD_DIR to the build output directory\n",
    "os.environ[\"FINN_BUILD_DIR\"] = os.path.abspath(\"../build\")\n",
    "\n",
    "print(f\"✓ FINN_BUILD_DIR set to: {os.environ['FINN_BUILD_DIR']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9dec8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1218 15:16:12.825714 6368 site-packages/torch/utils/cpp_extension.py:118] No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n",
      "  Python: 3.9.25 (main, Nov  3 2025, 22:33:05) \n",
      "[GCC 11.2.0]\n",
      "  Working directory: /home/hritik/Desktop/Hritik/Project/ellipse-regression-project/finn-kria-kv260-build/notebooks\n",
      "✓ FINN imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "# FINN imports\n",
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "from finn.builder.build_dataflow_config import DataflowBuildConfig\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"  Python: {sys.version}\")\n",
    "print(f\"  Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Test FINN\n",
    "try:\n",
    "    from finn.util.basic import get_finn_root\n",
    "    print(f\"✓ FINN root: {get_finn_root()}\")\n",
    "except:\n",
    "    print(\"✓ FINN imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1d3c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded: ../../finn_build/ellipse_qonnx_streamlined.qonnx\n",
      "  Input: global_in [0, 1, 20, 20]\n",
      "  Output: global_out [1, 5]\n",
      "  Total nodes: 63\n",
      "\n",
      "  Node type distribution:\n",
      "    Mul: 12\n",
      "    Where: 12\n",
      "    Relu: 6\n",
      "    Round: 6\n",
      "    Greater: 6\n",
      "    Less: 6\n",
      "    Conv: 4\n",
      "    MaxPool: 4\n",
      "    Add: 3\n",
      "    Gemm: 3\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Verify Model Exists\n",
    "\n",
    "# Path to your streamlined QONNX model\n",
    "model_path = \"../../finn_build/ellipse_qonnx_streamlined.qonnx\"\n",
    "\n",
    "# Check if model exists\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Model not found: {model_path}\\n\"\n",
    "        \"Please run 2-finn.ipynb first to generate the streamlined model.\"\n",
    "    )\n",
    "\n",
    "# Load and inspect the model\n",
    "model = ModelWrapper(model_path)\n",
    "print(f\"✓ Model loaded: {model_path}\")\n",
    "print(f\"  Input: {model.graph.input[0].name} {[d.dim_value for d in model.graph.input[0].type.tensor_type.shape.dim]}\")\n",
    "print(f\"  Output: {model.graph.output[0].name} {[d.dim_value for d in model.graph.output[0].type.tensor_type.shape.dim]}\")\n",
    "print(f\"  Total nodes: {len(model.graph.node)}\")\n",
    "\n",
    "# Count node types\n",
    "from collections import Counter\n",
    "node_types = Counter([n.op_type for n in model.graph.node])\n",
    "print(f\"\\n  Node type distribution:\")\n",
    "for op_type, count in sorted(node_types.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"    {op_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf1d3179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cleaning existing build directory: ../build\n",
      "✓ Build configuration:\n",
      "  Model: ../../finn_build/ellipse_qonnx_streamlined.qonnx\n",
      "  Output: ../build\n",
      "  Target board: Kria KV260\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Configure Build Settings\n",
    "\n",
    "# Output directory for build artifacts\n",
    "output_dir = \"../build\"\n",
    "\n",
    "# Clean output directory if it exists\n",
    "if os.path.exists(output_dir):\n",
    "    print(f\"  Cleaning existing build directory: {output_dir}\")\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Build configuration:\")\n",
    "print(f\"  Model: {model_path}\")\n",
    "print(f\"  Output: {output_dir}\")\n",
    "print(f\"  Target board: Kria KV260\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea74db49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Build configuration created (based on FINN cybersecurity example)\n",
      "  Target FPGA: xck26-sfvc784-2LV-c\n",
      "  Clock: 200 MHz\n",
      "\n",
      "  Build steps (16):\n",
      "     1. Qonnx To Finn\n",
      "     2. Tidy Up\n",
      "     3. Streamline\n",
      "     4. Convert To Hw\n",
      "     5. Target Fps Parallelization\n",
      "     6. Apply Folding Config\n",
      "     7. Minimize Bit Width\n",
      "     8. Generate Estimate Reports\n",
      "     9. Hw Codegen\n",
      "    10. Hw Ipgen\n",
      "    11. Set Fifo Depths\n",
      "    12. Create Stitched Ip\n",
      "    13. Out Of Context Synthesis\n",
      "    14. Synthesize Bitfile\n",
      "    15. Make Pynq Driver\n",
      "    16. Deployment Package\n",
      "\n",
      "  Output types:\n",
      "    - ESTIMATE_REPORTS\n",
      "    - STITCHED_IP\n",
      "    - RTLSIM_PERFORMANCE\n",
      "    - OOC_SYNTH\n",
      "    - BITFILE\n",
      "    - DEPLOYMENT_PACKAGE\n",
      "\n",
      "⚠️  Note: Partitioning step skipped (not required for this model)\n",
      "   This follows FINN cybersecurity example pattern\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create Dataflow Build Configuration (Based on FINN Cybersecurity Example)\n",
    "from finn.builder.build_dataflow_steps import (\n",
    "    step_qonnx_to_finn,\n",
    "    step_tidy_up,\n",
    "    step_streamline,\n",
    "    step_convert_to_hw,\n",
    "    # Skip step_create_dataflow_partition - causes cycle errors in some models\n",
    "    step_target_fps_parallelization,\n",
    "    step_apply_folding_config,\n",
    "    step_minimize_bit_width,\n",
    "    step_generate_estimate_reports,\n",
    "    step_hw_codegen,\n",
    "    step_hw_ipgen,\n",
    "    step_set_fifo_depths,\n",
    "    step_create_stitched_ip,\n",
    "    step_out_of_context_synthesis,\n",
    "    step_synthesize_bitfile,\n",
    "    step_make_pynq_driver,\n",
    "    step_deployment_package,\n",
    ")\n",
    "\n",
    "# Build steps following FINN cybersecurity example pattern\n",
    "build_steps = [\n",
    "    step_qonnx_to_finn,\n",
    "    step_tidy_up,\n",
    "    step_streamline,\n",
    "    step_convert_to_hw,\n",
    "    # Partitioning skipped - not needed for single-partition designs\n",
    "    step_target_fps_parallelization,\n",
    "    step_apply_folding_config,\n",
    "    step_minimize_bit_width,\n",
    "    step_generate_estimate_reports,\n",
    "    step_hw_codegen,\n",
    "    step_hw_ipgen,\n",
    "    step_set_fifo_depths,\n",
    "    step_create_stitched_ip,\n",
    "    step_out_of_context_synthesis,\n",
    "    step_synthesize_bitfile,\n",
    "    step_make_pynq_driver,\n",
    "    step_deployment_package,\n",
    "]\n",
    "\n",
    "# Kria KV260 configuration\n",
    "cfg = DataflowBuildConfig(\n",
    "    output_dir=output_dir,\n",
    "    # Target FPGA part for Kria KV260\n",
    "    fpga_part=\"xck26-sfvc784-2LV-c\",\n",
    "    # Clock frequency - 200 MHz (5.0 ns period)\n",
    "    synth_clk_period_ns=5.0,\n",
    "    # Custom build steps\n",
    "    steps=build_steps,\n",
    "    # Generate all outputs\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,\n",
    "        build_cfg.DataflowOutputType.STITCHED_IP,\n",
    "        build_cfg.DataflowOutputType.RTLSIM_PERFORMANCE,\n",
    "        build_cfg.DataflowOutputType.OOC_SYNTH,\n",
    "        build_cfg.DataflowOutputType.BITFILE,\n",
    "        build_cfg.DataflowOutputType.DEPLOYMENT_PACKAGE,\n",
    "    ],\n",
    "    # FIFO configuration (like cybersecurity example)\n",
    "    auto_fifo_depths=True,\n",
    "    auto_fifo_strategy=build_cfg.AutoFIFOSizingMethod.LARGEFIFO_RTLSIM,\n",
    "    # Folding configuration\n",
    "    folding_config_file=None,  # Auto-generate\n",
    "    # Shell flow for Zynq (required for KV260)\n",
    "    shell_flow_type=build_cfg.ShellFlowType.VIVADO_ZYNQ,\n",
    "    # Verbose logging\n",
    "    verbose=True,\n",
    "    save_intermediate_models=True,\n",
    "    # Standalone thresholds (helps with resource usage)\n",
    "    standalone_thresholds=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Build configuration created (based on FINN cybersecurity example)\")\n",
    "print(f\"  Target FPGA: {cfg.fpga_part}\")\n",
    "print(f\"  Clock: {1000/cfg.synth_clk_period_ns:.0f} MHz\")\n",
    "print(f\"\\n  Build steps ({len(cfg.steps)}):\")\n",
    "for i, step_func in enumerate(cfg.steps, 1):\n",
    "    step_name = step_func.__name__.replace('step_', '').replace('_', ' ').title()\n",
    "    print(f\"    {i:2d}. {step_name}\")\n",
    "\n",
    "print(f\"\\n  Output types:\")\n",
    "for output_type in cfg.generate_outputs:\n",
    "    print(f\"    - {output_type.name}\")\n",
    "\n",
    "print(\"\\n⚠️  Note: Partitioning step skipped (not required for this model)\")\n",
    "print(\"   This follows FINN cybersecurity example pattern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b550430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STARTING FINN DATAFLOW BUILD FOR KRIA KV260\n",
      "======================================================================\n",
      "\n",
      "Build started: 2025-12-18 15:16:58\n",
      "Model: ../../finn_build/ellipse_qonnx_streamlined.qonnx\n",
      "Output: ../build\n",
      "\n",
      "⚠️  This process will take 2-6 hours depending on model complexity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.1: Initialize Build\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING FINN DATAFLOW BUILD FOR KRIA KV260\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBuild started: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Model: {model_path}\")\n",
    "print(f\"Output: {output_dir}\")\n",
    "print(\"\\n⚠️  This process will take 2-6 hours depending on model complexity.\\n\")\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c94a17f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully imported finn.builder.build_dataflow\n",
      "✓ build_dataflow_cfg function found\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.2: Test FINN Build Import\n",
    "\n",
    "try:\n",
    "    import finn.builder.build_dataflow as build\n",
    "    print(\"✓ Successfully imported finn.builder.build_dataflow\")\n",
    "    \n",
    "    # Check if build_dataflow_cfg function exists\n",
    "    if hasattr(build, 'build_dataflow_cfg'):\n",
    "        print(\"✓ build_dataflow_cfg function found\")\n",
    "    else:\n",
    "        print(\"✗ build_dataflow_cfg function not found\")\n",
    "        print(f\"Available functions: {dir(build)}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error importing FINN build: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b1c622e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying inputs before build...\n",
      "\n",
      "1. Model path: ../../finn_build/ellipse_qonnx_streamlined.qonnx\n",
      "   Exists: True\n",
      "\n",
      "2. Output directory: ../build\n",
      "   Exists: True\n",
      "\n",
      "3. Config object: <class 'finn.builder.build_dataflow_config.DataflowBuildConfig'>\n",
      "   FPGA part: xck26-sfvc784-2LV-c\n",
      "   Clock period: 5.0 ns\n",
      "   Number of steps: 16\n",
      "   Output types: 6\n",
      "\n",
      "✓ Pre-build verification complete\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.3: Verify Model and Config\n",
    "\n",
    "print(\"Verifying inputs before build...\")\n",
    "print(f\"\\n1. Model path: {model_path}\")\n",
    "print(f\"   Exists: {os.path.exists(model_path)}\")\n",
    "\n",
    "print(f\"\\n2. Output directory: {output_dir}\")\n",
    "print(f\"   Exists: {os.path.exists(output_dir)}\")\n",
    "\n",
    "print(f\"\\n3. Config object: {type(cfg)}\")\n",
    "print(f\"   FPGA part: {cfg.fpga_part}\")\n",
    "print(f\"   Clock period: {cfg.synth_clk_period_ns} ns\")\n",
    "print(f\"   Number of steps: {len(cfg.steps)}\")\n",
    "print(f\"   Output types: {len(cfg.generate_outputs)}\")\n",
    "\n",
    "print(\"\\n✓ Pre-build verification complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "158799a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DIAGNOSING CONV NODES IN MODEL\n",
      "======================================================================\n",
      "\n",
      "Found 4 Conv node(s)\n",
      "\n",
      "1. Conv node: Conv_0\n",
      "   Input: ['global_in', 'Conv_0_param0', 'Conv_0_param1']\n",
      "   Output: ['Conv_0_out0']\n",
      "   Attributes:\n",
      "     - auto_pad: name: \"auto_pad\"\n",
      "s: \"NOTSET\"\n",
      "type: STRING\n",
      "\n",
      "     - dilations: name: \"dilations\"\n",
      "ints: 1\n",
      "ints: 1\n",
      "type: INTS\n",
      "\n",
      "     - group: name: \"group\"\n",
      "i: 1\n",
      "type: INT\n",
      "\n",
      "     - pads: name: \"pads\"\n",
      "ints: 1\n",
      "ints: 1\n",
      "ints: 1\n",
      "ints: 1\n",
      "type: INTS\n",
      "\n",
      "     - strides: name: \"strides\"\n",
      "ints: 1\n",
      "ints: 1\n",
      "type: INTS\n",
      "\n",
      "   ✗ MISSING kernel_shape\n",
      "   ✓ strides\n",
      "   ✓ pads\n",
      "   ✓ dilations\n",
      "   Weight shape: (32, 1, 3, 3)\n",
      "\n",
      "2. Conv node: Conv_1\n",
      "   Input: ['MaxPool_0_out0', 'Conv_1_param0']\n",
      "   Output: ['Conv_1_out0']\n",
      "   Attributes:\n",
      "     - auto_pad: name: \"auto_pad\"\n",
      "s: \"NOTSET\"\n",
      "type: STRING\n",
      "\n",
      "     - dilations: name: \"dilations\"\n",
      "ints: 1\n",
      "ints: 1\n",
      "type: INTS\n",
      "\n",
      "     - group: name: \"group\"\n",
      "i: 1\n",
      "type: INT\n",
      "\n",
      "     - pads: name: \"pads\"\n",
      "ints: 1\n",
      "ints: 1\n",
      "ints: 1\n",
      "ints: 1\n",
      "type: INTS\n",
      "\n",
      "     - strides: name: \"strides\"\n",
      "ints: 1\n",
      "ints: 1\n",
      "type: INTS\n",
      "\n",
      "   ✗ MISSING kernel_shape\n",
      "   ✓ strides\n",
      "   ✓ pads\n",
      "   ✓ dilations\n",
      "   Weight shape: (64, 32, 3, 3)\n",
      "\n",
      "3. Conv node: Conv_2\n",
      "   Input: ['MaxPool_1_out0', 'Conv_2_param0']\n",
      "   Output: ['Conv_2_out0']\n",
      "   Attributes:\n",
      "     - auto_pad: name: \"auto_pad\"\n",
      "s: \"NOTSET\"\n",
      "type: STRING\n",
      "\n",
      "     - dilations: name: \"dilations\"\n",
      "ints: 1\n",
      "ints: 1\n",
      "type: INTS\n",
      "\n",
      "     - group: name: \"group\"\n",
      "i: 1\n",
      "type: INT\n",
      "\n",
      "     - pads: name: \"pads\"\n",
      "ints: 1\n",
      "ints: 1\n",
      "ints: 1\n",
      "ints: 1\n",
      "type: INTS\n",
      "\n",
      "     - strides: name: \"strides\"\n",
      "ints: 1\n",
      "ints: 1\n",
      "type: INTS\n",
      "\n",
      "   ✗ MISSING kernel_shape\n",
      "   ✓ strides\n",
      "   ✓ pads\n",
      "   ✓ dilations\n",
      "   Weight shape: (128, 64, 3, 3)\n",
      "\n",
      "4. Conv node: Conv_3\n",
      "   Input: ['MaxPool_2_out0', 'Conv_3_param0']\n",
      "   Output: ['Conv_3_out0']\n",
      "   Attributes:\n",
      "     - auto_pad: name: \"auto_pad\"\n",
      "s: \"NOTSET\"\n",
      "type: STRING\n",
      "\n",
      "     - dilations: name: \"dilations\"\n",
      "ints: 1\n",
      "ints: 1\n",
      "type: INTS\n",
      "\n",
      "     - group: name: \"group\"\n",
      "i: 1\n",
      "type: INT\n",
      "\n",
      "     - pads: name: \"pads\"\n",
      "ints: 1\n",
      "ints: 1\n",
      "ints: 1\n",
      "ints: 1\n",
      "type: INTS\n",
      "\n",
      "     - strides: name: \"strides\"\n",
      "ints: 1\n",
      "ints: 1\n",
      "type: INTS\n",
      "\n",
      "   ✗ MISSING kernel_shape\n",
      "   ✓ strides\n",
      "   ✓ pads\n",
      "   ✓ dilations\n",
      "   Weight shape: (256, 128, 3, 3)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.4.1: Diagnose Conv Nodes\n",
    "\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DIAGNOSING CONV NODES IN MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = ModelWrapper(model_path)\n",
    "\n",
    "conv_nodes = [n for n in model.graph.node if n.op_type == 'Conv']\n",
    "print(f\"\\nFound {len(conv_nodes)} Conv node(s)\")\n",
    "\n",
    "for i, node in enumerate(conv_nodes, 1):\n",
    "    print(f\"\\n{i}. Conv node: {node.name}\")\n",
    "    print(f\"   Input: {node.input}\")\n",
    "    print(f\"   Output: {node.output}\")\n",
    "    print(f\"   Attributes:\")\n",
    "    \n",
    "    for attr in node.attribute:\n",
    "        print(f\"     - {attr.name}: {attr}\")\n",
    "    \n",
    "    # Check for missing required attributes\n",
    "    required_attrs = ['kernel_shape', 'strides', 'pads', 'dilations']\n",
    "    for req_attr in required_attrs:\n",
    "        has_attr = any(a.name == req_attr for a in node.attribute)\n",
    "        status = \"✓\" if has_attr else \"✗ MISSING\"\n",
    "        print(f\"   {status} {req_attr}\")\n",
    "    \n",
    "    # Check weight tensor\n",
    "    if len(node.input) > 1:\n",
    "        weight_name = node.input[1]\n",
    "        if model.get_initializer(weight_name) is not None:\n",
    "            weight = model.get_initializer(weight_name)\n",
    "            print(f\"   Weight shape: {weight.shape}\")\n",
    "        else:\n",
    "            print(f\"   ⚠ Weight not found in initializers\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc07e689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FIXING CONV NODES\n",
      "======================================================================\n",
      "\n",
      "Processing Conv node: Conv_0\n",
      "  ✓ Added kernel_shape: [3, 3]\n",
      "\n",
      "Processing Conv node: Conv_1\n",
      "  ✓ Added kernel_shape: [3, 3]\n",
      "\n",
      "Processing Conv node: Conv_2\n",
      "  ✓ Added kernel_shape: [3, 3]\n",
      "\n",
      "Processing Conv node: Conv_3\n",
      "  ✓ Added kernel_shape: [3, 3]\n",
      "\n",
      "✓ Fixed model saved to: ../../finn_build/ellipse_qonnx_streamlined_fixed.qonnx\n",
      "\n",
      "Update model_path to use the fixed model:\n",
      "  model_path = '../../finn_build/ellipse_qonnx_streamlined_fixed.qonnx'\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.4.2: Fix Conv Nodes (Add Missing Attributes)\n",
    "\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from onnx import helper\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FIXING CONV NODES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = ModelWrapper(model_path)\n",
    "modified = False\n",
    "\n",
    "for node in model.graph.node:\n",
    "    if node.op_type != 'Conv':\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessing Conv node: {node.name}\")\n",
    "    \n",
    "    # Helper to check if attribute exists\n",
    "    def has_attr(name):\n",
    "        return any(a.name == name for a in node.attribute)\n",
    "    \n",
    "    # Helper to get attribute value\n",
    "    def get_attr(name):\n",
    "        for a in node.attribute:\n",
    "            if a.name == name:\n",
    "                if a.ints:\n",
    "                    return list(a.ints)\n",
    "                elif a.floats:\n",
    "                    return list(a.floats)\n",
    "                elif a.i:\n",
    "                    return a.i\n",
    "        return None\n",
    "    \n",
    "    # Infer kernel_shape from weight tensor if missing\n",
    "    if not has_attr('kernel_shape'):\n",
    "        if len(node.input) > 1:\n",
    "            weight_name = node.input[1]\n",
    "            weight = model.get_initializer(weight_name)\n",
    "            if weight is not None:\n",
    "                # Weight shape: (M, C, kH, kW) for 2D conv\n",
    "                k_shape = list(weight.shape[2:4])\n",
    "                node.attribute.append(helper.make_attribute(\"kernel_shape\", k_shape))\n",
    "                print(f\"  ✓ Added kernel_shape: {k_shape}\")\n",
    "                modified = True\n",
    "            else:\n",
    "                print(f\"  ✗ Cannot infer kernel_shape - weight not found\")\n",
    "        else:\n",
    "            print(f\"  ✗ Cannot infer kernel_shape - no weight input\")\n",
    "    \n",
    "    # Add default strides if missing\n",
    "    if not has_attr('strides'):\n",
    "        strides = [1, 1]\n",
    "        node.attribute.append(helper.make_attribute(\"strides\", strides))\n",
    "        print(f\"  ✓ Added strides: {strides}\")\n",
    "        modified = True\n",
    "    \n",
    "    # Add default pads if missing\n",
    "    if not has_attr('pads'):\n",
    "        pads = [0, 0, 0, 0]\n",
    "        node.attribute.append(helper.make_attribute(\"pads\", pads))\n",
    "        print(f\"  ✓ Added pads: {pads}\")\n",
    "        modified = True\n",
    "    \n",
    "    # Add default dilations if missing\n",
    "    if not has_attr('dilations'):\n",
    "        dilations = [1, 1]\n",
    "        node.attribute.append(helper.make_attribute(\"dilations\", dilations))\n",
    "        print(f\"  ✓ Added dilations: {dilations}\")\n",
    "        modified = True\n",
    "    \n",
    "    # Add default group if missing\n",
    "    if not has_attr('group'):\n",
    "        node.attribute.append(helper.make_attribute(\"group\", 1))\n",
    "        print(f\"  ✓ Added group: 1\")\n",
    "        modified = True\n",
    "\n",
    "if modified:\n",
    "    # Save fixed model\n",
    "    fixed_model_path = model_path.replace('.qonnx', '_fixed.qonnx')\n",
    "    model.save(fixed_model_path)\n",
    "    print(f\"\\n✓ Fixed model saved to: {fixed_model_path}\")\n",
    "    print(f\"\\nUpdate model_path to use the fixed model:\")\n",
    "    print(f\"  model_path = '{fixed_model_path}'\")\n",
    "else:\n",
    "    print(f\"\\n✓ No modifications needed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3774aafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 intermediate models in ../build/intermediate_models/\n"
     ]
    }
   ],
   "source": [
    "# Inspect all intermediate models and their node types\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from collections import Counter\n",
    "import glob\n",
    "\n",
    "intermediate_dir = \"../build/intermediate_models/\"\n",
    "intermediate_models = sorted(glob.glob(f\"{intermediate_dir}/*.onnx\"))\n",
    "print(f\"Found {len(intermediate_models)} intermediate models in {intermediate_dir}\")\n",
    "\n",
    "for model_file in intermediate_models:\n",
    "    model = ModelWrapper(model_file)\n",
    "    node_types = Counter([n.op_type for n in model.graph.node])\n",
    "    print(f\"\\nModel: {model_file}\")\n",
    "    print(f\"  Total nodes: {len(model.graph.node)}\")\n",
    "    print(f\"  Node type distribution:\")\n",
    "    for op_type, count in sorted(node_types.items(), key=lambda x: -x[1]):\n",
    "        print(f\"    {op_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95f6d02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fixed model: ../../finn_build/ellipse_qonnx_streamlined_fixed.qonnx\n",
      "\n",
      "Model path: ../../finn_build/ellipse_qonnx_streamlined_fixed.qonnx\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.4.3: Update Model Path and Retry Build\n",
    "\n",
    "# If the previous cell created a fixed model, update the path\n",
    "import os\n",
    "\n",
    "fixed_model_path = model_path.replace('.qonnx', '_fixed.qonnx')\n",
    "\n",
    "if os.path.exists(fixed_model_path):\n",
    "    print(f\"Using fixed model: {fixed_model_path}\")\n",
    "    model_path = fixed_model_path\n",
    "else:\n",
    "    print(f\"Using original model: {model_path}\")\n",
    "\n",
    "print(f\"\\nModel path: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a83f6f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "RETRYING BUILD WITH FIXED MODEL\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Starting build with fixed model...\n",
      "Building dataflow accelerator from ../../finn_build/ellipse_qonnx_streamlined_fixed.qonnx\n",
      "Intermediate outputs will be generated in /home/hritik/Desktop/Hritik/Project/ellipse-regression-project/finn-kria-kv260-build/build\n",
      "Final outputs will be generated in ../build\n",
      "Build log is at ../build/build_dataflow.log\n",
      "Running step: step_qonnx_to_finn [1/3]\n",
      "Running step: step_tidy_up [2/3]\n",
      "Running step: step_streamline [3/3]\n",
      "Completed successfully\n",
      "\n",
      "✓ Build successful with fixed model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hritik/miniconda3/envs/ellipse-finn/lib/python3.9/site-packages/qonnx/transformation/infer_data_layouts.py:127: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.4.4: Retry Test Build with Fixed Model\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"RETRYING BUILD WITH FIXED MODEL\")\n",
    "print(\"-\"*70 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    from finn.builder.build_dataflow_config import DataflowBuildConfig\n",
    "    \n",
    "    test_cfg = DataflowBuildConfig(\n",
    "        output_dir=output_dir,\n",
    "        fpga_part=cfg.fpga_part,\n",
    "        synth_clk_period_ns=cfg.synth_clk_period_ns,\n",
    "        steps=cfg.steps[:3],\n",
    "        generate_outputs=[],\n",
    "        verbose=True,\n",
    "        save_intermediate_models=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting build with fixed model...\")\n",
    "    build.build_dataflow_cfg(model_path, test_cfg)\n",
    "    \n",
    "    print(\"\\n✓ Build successful with fixed model!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Build still failing\")\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c11dd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 intermediate models in ../build/intermediate_models/\n",
      "\n",
      "Model: ../build/intermediate_models/step_qonnx_to_finn.onnx\n",
      "  Total nodes: 63\n",
      "  Node type distribution:\n",
      "    Mul: 12\n",
      "    Where: 12\n",
      "    Relu: 6\n",
      "    Round: 6\n",
      "    Greater: 6\n",
      "    Less: 6\n",
      "    Conv: 4\n",
      "    MaxPool: 4\n",
      "    Add: 3\n",
      "    Gemm: 3\n",
      "    Reshape: 1\n",
      "\n",
      "Model: ../build/intermediate_models/step_streamline.onnx\n",
      "  Total nodes: 76\n",
      "  Node type distribution:\n",
      "    Mul: 12\n",
      "    Where: 12\n",
      "    Transpose: 8\n",
      "    Relu: 6\n",
      "    Round: 6\n",
      "    Greater: 6\n",
      "    Less: 6\n",
      "    Im2Col: 4\n",
      "    MatMul: 4\n",
      "    Add: 4\n",
      "    MaxPoolNHWC: 3\n",
      "    Gemm: 3\n",
      "    MaxPool: 1\n",
      "    Reshape: 1\n",
      "\n",
      "Model: ../build/intermediate_models/step_tidy_up.onnx\n",
      "  Total nodes: 63\n",
      "  Node type distribution:\n",
      "    Mul: 12\n",
      "    Where: 12\n",
      "    Relu: 6\n",
      "    Round: 6\n",
      "    Greater: 6\n",
      "    Less: 6\n",
      "    Conv: 4\n",
      "    MaxPool: 4\n",
      "    Add: 3\n",
      "    Gemm: 3\n",
      "    Reshape: 1\n"
     ]
    }
   ],
   "source": [
    "# Inspect all intermediate models and their node types\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from collections import Counter\n",
    "import glob\n",
    "\n",
    "intermediate_dir = \"../build/intermediate_models/\"\n",
    "intermediate_models = sorted(glob.glob(f\"{intermediate_dir}/*.onnx\"))\n",
    "print(f\"Found {len(intermediate_models)} intermediate models in {intermediate_dir}\")\n",
    "\n",
    "for model_file in intermediate_models:\n",
    "    model = ModelWrapper(model_file)\n",
    "    node_types = Counter([n.op_type for n in model.graph.node])\n",
    "    print(f\"\\nModel: {model_file}\")\n",
    "    print(f\"  Total nodes: {len(model.graph.node)}\")\n",
    "    print(f\"  Node type distribution:\")\n",
    "    for op_type, count in sorted(node_types.items(), key=lambda x: -x[1]):\n",
    "        print(f\"    {op_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e07f85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../build/intermediate_models/step_convert_to_hw.onnx not found.\n"
     ]
    }
   ],
   "source": [
    "# Inspect step_convert_to_hw.onnx if it exists\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "model_file = \"../build/intermediate_models/step_convert_to_hw.onnx\"\n",
    "if os.path.exists(model_file):\n",
    "    model = ModelWrapper(model_file)\n",
    "    node_types = Counter([n.op_type for n in model.graph.node])\n",
    "    print(f\"Model: {model_file}\")\n",
    "    print(f\"  Total nodes: {len(model.graph.node)}\")\n",
    "    print(f\"  Node type distribution:\")\n",
    "    for op_type, count in sorted(node_types.items(), key=lambda x: -x[1]):\n",
    "        print(f\"    {op_type}: {count}\")\n",
    "else:\n",
    "    print(f\"{model_file} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ccf730b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RUNNING: step_convert_to_hw\n",
      "======================================================================\n",
      "\n",
      "Building dataflow accelerator from ../../finn_build/ellipse_qonnx_streamlined_fixed.qonnx\n",
      "Intermediate outputs will be generated in /home/hritik/Desktop/Hritik/Project/ellipse-regression-project/finn-kria-kv260-build/build\n",
      "Final outputs will be generated in ../build\n",
      "Build log is at ../build/build_dataflow.log\n",
      "Running step: step_convert_to_hw [1/1]\n",
      "Completed successfully\n",
      "\n",
      "✓ step_convert_to_hw completed!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.4.5: Run step_convert_to_hw\n",
    "from finn.builder.build_dataflow_steps import step_convert_to_hw\n",
    "from finn.builder.build_dataflow_config import DataflowBuildConfig\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING: step_convert_to_hw\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    convert_cfg = DataflowBuildConfig(\n",
    "        output_dir=output_dir,\n",
    "        fpga_part=cfg.fpga_part,\n",
    "        synth_clk_period_ns=cfg.synth_clk_period_ns,\n",
    "        steps=[step_convert_to_hw],\n",
    "        generate_outputs=[],\n",
    "        verbose=True,\n",
    "        save_intermediate_models=True,\n",
    "    )\n",
    "    build.build_dataflow_cfg(model_path, convert_cfg)\n",
    "    print(\"\\n✓ step_convert_to_hw completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ step_convert_to_hw failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "002cdbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 intermediate models in ../build/intermediate_models/\n",
      "\n",
      "Model: ../build/intermediate_models/step_convert_to_hw.onnx\n",
      "  Total nodes: 63\n",
      "  Node type distribution:\n",
      "    Mul: 12\n",
      "    Where: 12\n",
      "    Relu: 6\n",
      "    Round: 6\n",
      "    Greater: 6\n",
      "    Less: 6\n",
      "    Conv: 4\n",
      "    MaxPool: 4\n",
      "    Add: 3\n",
      "    Gemm: 3\n",
      "    Reshape: 1\n",
      "\n",
      "Model: ../build/intermediate_models/step_qonnx_to_finn.onnx\n",
      "  Total nodes: 63\n",
      "  Node type distribution:\n",
      "    Mul: 12\n",
      "    Where: 12\n",
      "    Relu: 6\n",
      "    Round: 6\n",
      "    Greater: 6\n",
      "    Less: 6\n",
      "    Conv: 4\n",
      "    MaxPool: 4\n",
      "    Add: 3\n",
      "    Gemm: 3\n",
      "    Reshape: 1\n",
      "\n",
      "Model: ../build/intermediate_models/step_streamline.onnx\n",
      "  Total nodes: 76\n",
      "  Node type distribution:\n",
      "    Mul: 12\n",
      "    Where: 12\n",
      "    Transpose: 8\n",
      "    Relu: 6\n",
      "    Round: 6\n",
      "    Greater: 6\n",
      "    Less: 6\n",
      "    Im2Col: 4\n",
      "    MatMul: 4\n",
      "    Add: 4\n",
      "    MaxPoolNHWC: 3\n",
      "    Gemm: 3\n",
      "    MaxPool: 1\n",
      "    Reshape: 1\n",
      "\n",
      "Model: ../build/intermediate_models/step_tidy_up.onnx\n",
      "  Total nodes: 63\n",
      "  Node type distribution:\n",
      "    Mul: 12\n",
      "    Where: 12\n",
      "    Relu: 6\n",
      "    Round: 6\n",
      "    Greater: 6\n",
      "    Less: 6\n",
      "    Conv: 4\n",
      "    MaxPool: 4\n",
      "    Add: 3\n",
      "    Gemm: 3\n",
      "    Reshape: 1\n"
     ]
    }
   ],
   "source": [
    "# Inspect all intermediate models and their node types\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from collections import Counter\n",
    "import glob\n",
    "\n",
    "intermediate_dir = \"../build/intermediate_models/\"\n",
    "intermediate_models = sorted(glob.glob(f\"{intermediate_dir}/*.onnx\"))\n",
    "print(f\"Found {len(intermediate_models)} intermediate models in {intermediate_dir}\")\n",
    "\n",
    "for model_file in intermediate_models:\n",
    "    model = ModelWrapper(model_file)\n",
    "    node_types = Counter([n.op_type for n in model.graph.node])\n",
    "    print(f\"\\nModel: {model_file}\")\n",
    "    print(f\"  Total nodes: {len(model.graph.node)}\")\n",
    "    print(f\"  Node type distribution:\")\n",
    "    for op_type, count in sorted(node_types.items(), key=lambda x: -x[1]):\n",
    "        print(f\"    {op_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d884dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ../build/intermediate_models/step_convert_to_hw.onnx\n",
      "  Total nodes: 63\n",
      "  Node type distribution:\n",
      "    Mul: 12\n",
      "    Where: 12\n",
      "    Relu: 6\n",
      "    Round: 6\n",
      "    Greater: 6\n",
      "    Less: 6\n",
      "    Conv: 4\n",
      "    MaxPool: 4\n",
      "    Add: 3\n",
      "    Gemm: 3\n",
      "    Reshape: 1\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.4.6: Inspect step_convert_to_hw.onnx\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "model_file = \"../build/intermediate_models/step_convert_to_hw.onnx\"\n",
    "if os.path.exists(model_file):\n",
    "    model = ModelWrapper(model_file)\n",
    "    node_types = Counter([n.op_type for n in model.graph.node])\n",
    "    print(f\"Model: {model_file}\")\n",
    "    print(f\"  Total nodes: {len(model.graph.node)}\")\n",
    "    print(f\"  Node type distribution:\")\n",
    "    for op_type, count in sorted(node_types.items(), key=lambda x: -x[1]):\n",
    "        print(f\"    {op_type}: {count}\")\n",
    "else:\n",
    "    print(f\"{model_file} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35395705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate models: ['../build/intermediate_models/step_qonnx_to_finn.onnx', '../build/intermediate_models/step_streamline.onnx', '../build/intermediate_models/step_tidy_up.onnx']\n",
      "Model: ../build/intermediate_models/step_tidy_up.onnx\n",
      "  Total nodes: 63\n",
      "  Node type distribution:\n",
      "    Mul: 12\n",
      "    Where: 12\n",
      "    Relu: 6\n",
      "    Round: 6\n",
      "    Greater: 6\n",
      "    Less: 6\n",
      "    Conv: 4\n",
      "    MaxPool: 4\n",
      "    Add: 3\n",
      "    Gemm: 3\n",
      "    Reshape: 1\n"
     ]
    }
   ],
   "source": [
    "# Inspect intermediate model before estimate reports\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from collections import Counter\n",
    "import glob\n",
    "\n",
    "interm_dir = \"../build/intermediate_models/\"\n",
    "interm_models = sorted(glob.glob(f\"{interm_dir}/*.onnx\"))\n",
    "print(f\"Intermediate models: {interm_models[-3:]}\")  # Show last 3\n",
    "\n",
    "# Pick the latest model before the error\n",
    "if interm_models:\n",
    "    model = ModelWrapper(interm_models[-1])\n",
    "    node_types = Counter([n.op_type for n in model.graph.node])\n",
    "    print(f\"Model: {interm_models[-1]}\")\n",
    "    print(f\"  Total nodes: {len(model.graph.node)}\")\n",
    "    print(f\"  Node type distribution:\")\n",
    "    for op_type, count in sorted(node_types.items(), key=lambda x: -x[1]):\n",
    "        print(f\"    {op_type}: {count}\")\n",
    "else:\n",
    "    print(\"No intermediate models found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d3d870f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique op types in step_convert_to_hw.onnx: {'MaxPool', 'Conv', 'Mul', 'Greater', 'Round', 'Reshape', 'Gemm', 'Add', 'Less', 'Where', 'Relu'}\n",
      "⚠️  No FINN hardware nodes found. Your model may not be quantized or is not compatible with FINN hardware conversion.\n",
      "    Please ensure your model is quantized and uses only FINN-supported layers.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.4.7: Check for FINN hardware nodes in step_convert_to_hw.onnx\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "model_file = \"../build/intermediate_models/step_convert_to_hw.onnx\"\n",
    "if os.path.exists(model_file):\n",
    "    model = ModelWrapper(model_file)\n",
    "    op_types = set([n.op_type for n in model.graph.node])\n",
    "    print(\"Unique op types in step_convert_to_hw.onnx:\", op_types)\n",
    "    # List common FINN hardware node types\n",
    "    finn_hw_ops = [\n",
    "        \"StreamingFCLayer_Batch\", \"StreamingMaxPool_Batch\", \"StreamingConv\", \n",
    "        \"StreamingDataflowPartition\", \"Thresholding_Batch\", \"MultiThreshold\", \"TLastMarker\"\n",
    "    ]\n",
    "    found_hw = [op for op in op_types if op in finn_hw_ops]\n",
    "    if found_hw:\n",
    "        print(\"✓ FINN hardware nodes found:\", found_hw)\n",
    "    else:\n",
    "        print(\"⚠️  No FINN hardware nodes found. Your model may not be quantized or is not compatible with FINN hardware conversion.\")\n",
    "        print(\"    Please ensure your model is quantized and uses only FINN-supported layers.\")\n",
    "else:\n",
    "    print(f\"{model_file} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db258f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DIAGNOSIS: No FINN hardware nodes found after step_convert_to_hw\n",
      "======================================================================\n",
      "\n",
      "Possible reasons:\n",
      "  1. The model is not quantized (weights/activations are floating-point).\n",
      "  2. The model contains unsupported ONNX ops for FINN hardware.\n",
      "  3. The export from Brevitas or quantization step was skipped or failed.\n",
      "\n",
      "Recommended actions:\n",
      "  - Ensure you use Brevitas quantized layers (QuantConv2d, QuantLinear, etc.).\n",
      "  - Export your model to QONNX using Brevitas export.\n",
      "  - After export, check that all weights and activations are quantized (int2, int4, int8, etc.).\n",
      "  - The model should contain only FINN-supported ops (see: https://finn.readthedocs.io/en/latest/onnx_supported_ops.html).\n",
      "\n",
      "To check quantization, you can inspect the data types of weights in your model:\n",
      "\n",
      "Conv node 'Conv_0': weights dtype = float32, shape = (32, 1, 3, 3)\n",
      "Conv node 'Conv_1': weights dtype = float32, shape = (64, 32, 3, 3)\n",
      "Conv node 'Conv_2': weights dtype = float32, shape = (128, 64, 3, 3)\n",
      "Conv node 'Conv_3': weights dtype = float32, shape = (256, 128, 3, 3)\n",
      "Gemm node 'Gemm_0': weights dtype = float32, shape = (512, 256)\n",
      "Gemm node 'Gemm_1': weights dtype = float32, shape = (256, 512)\n",
      "Gemm node 'Gemm_2': weights dtype = float32, shape = (5, 256)\n",
      "\n",
      "If you see 'float32' weights, your model is NOT quantized. You must retrain/export with quantization.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.4.8: Diagnose why hardware nodes are missing\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DIAGNOSIS: No FINN hardware nodes found after step_convert_to_hw\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Possible reasons:\n",
    "  1. The model is not quantized (weights/activations are floating-point).\n",
    "  2. The model contains unsupported ONNX ops for FINN hardware.\n",
    "  3. The export from Brevitas or quantization step was skipped or failed.\n",
    "\n",
    "Recommended actions:\n",
    "  - Ensure you use Brevitas quantized layers (QuantConv2d, QuantLinear, etc.).\n",
    "  - Export your model to QONNX using Brevitas export.\n",
    "  - After export, check that all weights and activations are quantized (int2, int4, int8, etc.).\n",
    "  - The model should contain only FINN-supported ops (see: https://finn.readthedocs.io/en/latest/onnx_supported_ops.html).\n",
    "\n",
    "To check quantization, you can inspect the data types of weights in your model:\n",
    "\"\"\")\n",
    "\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "import numpy as np\n",
    "\n",
    "model_file = \"../build/intermediate_models/step_tidy_up.onnx\"\n",
    "model = ModelWrapper(model_file)\n",
    "for n in model.graph.node:\n",
    "    if n.op_type in [\"Conv\", \"Gemm\"]:\n",
    "        w_name = n.input[1]\n",
    "        w_tensor = model.get_initializer(w_name)\n",
    "        if w_tensor is not None:\n",
    "            print(f\"{n.op_type} node '{n.name}': weights dtype = {w_tensor.dtype}, shape = {w_tensor.shape}\")\n",
    "        else:\n",
    "            print(f\"{n.op_type} node '{n.name}': weights not found in initializers\")\n",
    "\n",
    "print(\"\\nIf you see 'float32' weights, your model is NOT quantized. You must retrain/export with quantization.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a31dad7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'qonnx.transformation.create_dataflow_partition'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cell: Attempt to partition and convert to hardware nodes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqonnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcreate_dataflow_partition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CreateDataflowPartition\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfinn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfpgadataflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert_to_hls_layers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConvertToHLSLayers\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqonnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelwrapper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelWrapper\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'qonnx.transformation.create_dataflow_partition'"
     ]
    }
   ],
   "source": [
    "# Cell: Attempt to partition and convert to hardware nodes\n",
    "from qonnx.transformation.create_dataflow_partition import CreateDataflowPartition\n",
    "from finn.transformation.fpgadataflow.convert_to_hls_layers import ConvertToHLSLayers\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "# Load the latest streamlined or tidy model\n",
    "hw_model_path = \"../build/intermediate_models/step_tidy_up.onnx\"\n",
    "model = ModelWrapper(hw_model_path)\n",
    "\n",
    "# Try to partition the model for dataflow\n",
    "model = model.transform(CreateDataflowPartition())\n",
    "\n",
    "# Try to convert to HLS layers (hardware nodes)\n",
    "model = model.transform(ConvertToHLSLayers())\n",
    "\n",
    "# Save the hardware-mapped model\n",
    "hw_out_path = \"../build/intermediate_models/manual_hw_partition.onnx\"\n",
    "model.save(hw_out_path)\n",
    "print(f\"✓ Hardware-mapped model saved to: {hw_out_path}\")\n",
    "\n",
    "# Inspect node types\n",
    "from collections import Counter\n",
    "node_types = Counter([n.op_type for n in model.graph.node])\n",
    "print(f\"  Total nodes: {len(model.graph.node)}\")\n",
    "print(f\"  Node type distribution:\")\n",
    "for op_type, count in sorted(node_types.items(), key=lambda x: -x[1]):\n",
    "    print(f\"    {op_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "573336db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique op types: {'Greater', 'Mul', 'Conv', 'Where', 'Less', 'MaxPool', 'Add', 'Reshape', 'Relu', 'Gemm', 'Round'}\n"
     ]
    }
   ],
   "source": [
    "# List all unique op types in your model\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "model = ModelWrapper(\"../build/intermediate_models/step_tidy_up.onnx\")\n",
    "op_types = set([n.op_type for n in model.graph.node])\n",
    "print(\"Unique op types:\", op_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee6fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RUNNING: Remaining FINN build steps (hardware generation to deployment)\n",
      "======================================================================\n",
      "\n",
      "Building dataflow accelerator from ../build/intermediate_models/step_convert_to_hw.onnx\n",
      "Intermediate outputs will be generated in /home/hritik/Desktop/Hritik/Project/ellipse-regression-project/finn-kria-kv260-build/build\n",
      "Final outputs will be generated in ../build\n",
      "Build log is at ../build/build_dataflow.log\n",
      "Running step: step_target_fps_parallelization [1/12]\n",
      "Running step: step_apply_folding_config [2/12]\n",
      "Running step: step_minimize_bit_width [3/12]\n",
      "Running step: step_generate_estimate_reports [4/12]\n",
      "> \u001b[0;32m/home/hritik/miniconda3/envs/ellipse-finn/lib/python3.9/site-packages/finn/analysis/fpgadataflow/dataflow_performance.py\u001b[0m(73)\u001b[0;36mdataflow_performance\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     71 \u001b[0;31m                    \u001b[0mmax_pred_latency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_latencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     72 \u001b[0;31m                \u001b[0mlatency_at_node_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_cycles\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax_pred_latency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 73 \u001b[0;31m    \u001b[0mcritical_path_cycles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatency_at_node_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     74 \u001b[0;31m    return {\n",
      "\u001b[0m\u001b[0;32m     75 \u001b[0;31m        \u001b[0;34m\"critical_path_cycles\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritical_path_cycles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/hritik/miniconda3/envs/ellipse-finn/lib/python3.9/site-packages/finn/builder/build_dataflow.py\", line 158, in build_dataflow_cfg\n",
      "    model = transform_step(model, cfg)\n",
      "  File \"/home/hritik/miniconda3/envs/ellipse-finn/lib/python3.9/site-packages/finn/builder/build_dataflow_steps.py\", line 487, in step_generate_estimate_reports\n",
      "    estimate_network_performance = model.analysis(dataflow_performance)\n",
      "  File \"/home/hritik/miniconda3/envs/ellipse-finn/lib/python3.9/site-packages/qonnx/core/modelwrapper.py\", line 129, in analysis\n",
      "    return analysis_fxn(self)\n",
      "  File \"/home/hritik/miniconda3/envs/ellipse-finn/lib/python3.9/site-packages/finn/analysis/fpgadataflow/dataflow_performance.py\", line 73, in dataflow_performance\n",
      "    critical_path_cycles = max(latency_at_node_output.values())\n",
      "ValueError: max() arg is an empty sequence\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.4.7: Run remaining build steps (hardware generation to deployment)\n",
    "from finn.builder.build_dataflow_steps import (\n",
    "    step_target_fps_parallelization,\n",
    "    step_apply_folding_config,\n",
    "    step_minimize_bit_width,\n",
    "    step_generate_estimate_reports,\n",
    "    step_hw_codegen,\n",
    "    step_hw_ipgen,\n",
    "    step_set_fifo_depths,\n",
    "    step_create_stitched_ip,\n",
    "    step_out_of_context_synthesis,\n",
    "    step_synthesize_bitfile,\n",
    "    step_make_pynq_driver,\n",
    "    step_deployment_package,\n",
    ")\n",
    "from finn.builder.build_dataflow_config import DataflowBuildConfig\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING: Remaining FINN build steps (hardware generation to deployment)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "remaining_steps = [\n",
    "    step_target_fps_parallelization,\n",
    "    step_apply_folding_config,\n",
    "    step_minimize_bit_width,\n",
    "    step_generate_estimate_reports,\n",
    "    step_hw_codegen,\n",
    "    step_hw_ipgen,\n",
    "    step_set_fifo_depths,\n",
    "    step_create_stitched_ip,\n",
    "    step_out_of_context_synthesis,\n",
    "    step_synthesize_bitfile,\n",
    "    step_make_pynq_driver,\n",
    "    step_deployment_package,\n",
    "]\n",
    "\n",
    "try:\n",
    "    hw_cfg = DataflowBuildConfig(\n",
    "        output_dir=output_dir,\n",
    "        fpga_part=cfg.fpga_part,\n",
    "        synth_clk_period_ns=cfg.synth_clk_period_ns,\n",
    "        steps=remaining_steps,\n",
    "        generate_outputs=[\n",
    "            build_cfg.DataflowOutputType.ESTIMATE_REPORTS,\n",
    "            build_cfg.DataflowOutputType.STITCHED_IP,\n",
    "            build_cfg.DataflowOutputType.RTLSIM_PERFORMANCE,\n",
    "            build_cfg.DataflowOutputType.OOC_SYNTH,\n",
    "            build_cfg.DataflowOutputType.BITFILE,\n",
    "            build_cfg.DataflowOutputType.DEPLOYMENT_PACKAGE,\n",
    "        ],\n",
    "        auto_fifo_depths=True,\n",
    "        auto_fifo_strategy=build_cfg.AutoFIFOSizingMethod.LARGEFIFO_RTLSIM,\n",
    "        folding_config_file=None,\n",
    "        shell_flow_type=build_cfg.ShellFlowType.VIVADO_ZYNQ,\n",
    "        verbose=True,\n",
    "        save_intermediate_models=True,\n",
    "        standalone_thresholds=True,\n",
    "    )\n",
    "    # Use the output of step_convert_to_hw as input\n",
    "    convert_model_path = \"../build/intermediate_models/step_convert_to_hw.onnx\"\n",
    "    build.build_dataflow_cfg(convert_model_path, hw_cfg)\n",
    "    print(\"\\n✓ Remaining build steps completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Remaining build steps failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b400c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RUNNING FULL BUILD - BITSTREAM GENERATION\n",
      "======================================================================\n",
      "\n",
      "Starting full build (bitstream generation)...\n",
      "  ✓ All steps will be run, including bitstream synthesis\n",
      "  ⏱️  Estimated time: 2-6 hours\n",
      "\n",
      "Building dataflow accelerator from ../../finn_build/ellipse_qonnx_streamlined_fixed.qonnx\n",
      "Intermediate outputs will be generated in /home/hritik/Desktop/Hritik/Project/ellipse-regression-project/finn-kria-kv260-build/build\n",
      "Final outputs will be generated in ../build\n",
      "Build log is at ../build/build_dataflow.log\n",
      "Running step: step_qonnx_to_finn [1/16]\n",
      "Running step: step_tidy_up [2/16]\n",
      "Running step: step_streamline [3/16]\n",
      "Running step: step_convert_to_hw [4/16]\n",
      "Running step: step_target_fps_parallelization [5/16]\n",
      "Running step: step_apply_folding_config [6/16]\n",
      "Running step: step_minimize_bit_width [7/16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hritik/miniconda3/envs/ellipse-finn/lib/python3.9/site-packages/finn/transformation/fpgadataflow/convert_to_hw_layers.py:63: UserWarning: Im2Col_0 : Input is not int. Can't infer ConvInpGen.\n",
      "  warnings.warn(\"%s : Input is not int. Can't infer ConvInpGen.\" % n.name)\n",
      "/home/hritik/miniconda3/envs/ellipse-finn/lib/python3.9/site-packages/finn/transformation/fpgadataflow/convert_to_hw_layers.py:63: UserWarning: Im2Col_1 : Input is not int. Can't infer ConvInpGen.\n",
      "  warnings.warn(\"%s : Input is not int. Can't infer ConvInpGen.\" % n.name)\n",
      "/home/hritik/miniconda3/envs/ellipse-finn/lib/python3.9/site-packages/finn/transformation/fpgadataflow/convert_to_hw_layers.py:63: UserWarning: Im2Col_2 : Input is not int. Can't infer ConvInpGen.\n",
      "  warnings.warn(\"%s : Input is not int. Can't infer ConvInpGen.\" % n.name)\n",
      "/home/hritik/miniconda3/envs/ellipse-finn/lib/python3.9/site-packages/finn/transformation/fpgadataflow/convert_to_hw_layers.py:63: UserWarning: Im2Col_3 : Input is not int. Can't infer ConvInpGen.\n",
      "  warnings.warn(\"%s : Input is not int. Can't infer ConvInpGen.\" % n.name)\n",
      "/home/hritik/miniconda3/envs/ellipse-finn/lib/python3.9/site-packages/finn/transformation/fpgadataflow/convert_to_hw_layers.py:427: UserWarning: MaxPoolNHWC_2: could not convert to HW\n",
      "  warnings.warn(node.name + \": could not convert to HW\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step: step_generate_estimate_reports [8/16]\n",
      "> \u001b[0;32m/home/hritik/miniconda3/envs/ellipse-finn/lib/python3.9/site-packages/finn/analysis/fpgadataflow/dataflow_performance.py\u001b[0m(73)\u001b[0;36mdataflow_performance\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     71 \u001b[0;31m                    \u001b[0mmax_pred_latency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_latencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     72 \u001b[0;31m                \u001b[0mlatency_at_node_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_cycles\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax_pred_latency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 73 \u001b[0;31m    \u001b[0mcritical_path_cycles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatency_at_node_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     74 \u001b[0;31m    return {\n",
      "\u001b[0m\u001b[0;32m     75 \u001b[0;31m        \u001b[0;34m\"critical_path_cycles\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritical_path_cycles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# --- Replace Cell 5.5.1 with the following to run the full build and generate the bitstream ---\n",
    "\n",
    "# Cell 5.5.1: Run Build - Generate Bitstream (FULL BUILD)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING FULL BUILD - BITSTREAM GENERATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    from finn.builder.build_dataflow_config import DataflowBuildConfig\n",
    "    from finn.builder.build_dataflow_steps import (\n",
    "        step_qonnx_to_finn,\n",
    "        step_tidy_up,\n",
    "        step_streamline,\n",
    "        step_convert_to_hw,\n",
    "        step_target_fps_parallelization,\n",
    "        step_apply_folding_config,\n",
    "        step_minimize_bit_width,\n",
    "        step_generate_estimate_reports,\n",
    "        step_hw_codegen,\n",
    "        step_hw_ipgen,\n",
    "        step_set_fifo_depths,\n",
    "        step_create_stitched_ip,\n",
    "        step_out_of_context_synthesis,\n",
    "        step_synthesize_bitfile,\n",
    "        step_make_pynq_driver,\n",
    "        step_deployment_package,\n",
    "    )\n",
    "\n",
    "    # Configuration - FULL build (all steps, including bitstream)\n",
    "    full_cfg = DataflowBuildConfig(\n",
    "        output_dir=output_dir,\n",
    "        fpga_part=\"xck26-sfvc784-2LV-c\",\n",
    "        synth_clk_period_ns=5.0,\n",
    "        steps=[\n",
    "            step_qonnx_to_finn,           # 1\n",
    "            step_tidy_up,                 # 2\n",
    "            step_streamline,              # 3\n",
    "            step_convert_to_hw,           # 4\n",
    "            step_target_fps_parallelization,  # 5\n",
    "            step_apply_folding_config,    # 6\n",
    "            step_minimize_bit_width,      # 7\n",
    "            step_generate_estimate_reports, # 8\n",
    "            step_hw_codegen,              # 9\n",
    "            step_hw_ipgen,                # 10\n",
    "            step_set_fifo_depths,         # 11\n",
    "            step_create_stitched_ip,      # 12\n",
    "            step_out_of_context_synthesis,# 13\n",
    "            step_synthesize_bitfile,      # 14\n",
    "            step_make_pynq_driver,        # 15\n",
    "            step_deployment_package,      # 16\n",
    "        ],\n",
    "        generate_outputs=[\n",
    "            build_cfg.DataflowOutputType.ESTIMATE_REPORTS,\n",
    "            build_cfg.DataflowOutputType.STITCHED_IP,\n",
    "            build_cfg.DataflowOutputType.RTLSIM_PERFORMANCE,\n",
    "            build_cfg.DataflowOutputType.OOC_SYNTH,\n",
    "            build_cfg.DataflowOutputType.BITFILE,\n",
    "            build_cfg.DataflowOutputType.DEPLOYMENT_PACKAGE,\n",
    "        ],\n",
    "        auto_fifo_depths=True,\n",
    "        auto_fifo_strategy=build_cfg.AutoFIFOSizingMethod.LARGEFIFO_RTLSIM,\n",
    "        folding_config_file=None,\n",
    "        shell_flow_type=build_cfg.ShellFlowType.VIVADO_ZYNQ,\n",
    "        verbose=True,\n",
    "        save_intermediate_models=True,\n",
    "        standalone_thresholds=True,\n",
    "    )\n",
    "\n",
    "    print(\"Starting full build (bitstream generation)...\")\n",
    "    print(\"  ✓ All steps will be run, including bitstream synthesis\")\n",
    "    print(\"  ⏱️  Estimated time: 2-6 hours\\n\")\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    build.build_dataflow_cfg(model_path, full_cfg)\n",
    "\n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✓ FULL BUILD COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n  Build time: {elapsed:.1f} minutes\")\n",
    "    print(f\"\\n  Bitstream and deployment package generated in:\")\n",
    "    print(f\"    {output_dir}/\")\n",
    "    print(f\"\\n  Next steps:\")\n",
    "    print(f\"    1. Check the build results and deployment package.\")\n",
    "    print(f\"    2. Copy the files to your KV260 board.\")\n",
    "    print(f\"    3. Use the provided driver and test script for inference.\")\n",
    "    print(\"\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Build failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5103bf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHECKING GENERATED IP CORES FROM STEP 9\n",
      "======================================================================\n",
      "\n",
      "✓ Found intermediate_models directory\n",
      "\n",
      "✗ No IP directories found\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "SEARCHING FOR IP PACKAGES (.zip)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "⚠️  No .zip IP packages found\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.5.2: Check Generated IP Cores (After Step 9)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHECKING GENERATED IP CORES FROM STEP 9\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "build_path = Path(output_dir)\n",
    "\n",
    "# Look for intermediate models directory\n",
    "intermediate_dir = build_path / \"intermediate_models\"\n",
    "if intermediate_dir.exists():\n",
    "    print(f\"\\n✓ Found intermediate_models directory\")\n",
    "    \n",
    "    # Find all IP-related directories\n",
    "    ip_dirs = []\n",
    "    for item in intermediate_dir.rglob(\"*\"):\n",
    "        if item.is_dir() and (\"ipgen\" in item.name or item.name == \"ip\"):\n",
    "            ip_dirs.append(item)\n",
    "    \n",
    "    if ip_dirs:\n",
    "        print(f\"\\n✓ Found {len(ip_dirs)} IP generation directories:\")\n",
    "        for ip_dir in sorted(ip_dirs):\n",
    "            rel_path = ip_dir.relative_to(build_path)\n",
    "            print(f\"\\n  {rel_path}/\")\n",
    "            \n",
    "            # List IP files\n",
    "            ip_files = list(ip_dir.glob(\"*.zip\")) + list(ip_dir.glob(\"*.tcl\")) + list(ip_dir.glob(\"component.xml\"))\n",
    "            if ip_files:\n",
    "                print(f\"    Contains {len(ip_files)} IP files:\")\n",
    "                for f in ip_files[:5]:\n",
    "                    print(f\"      - {f.name}\")\n",
    "                if len(ip_files) > 5:\n",
    "                    print(f\"      ... and {len(ip_files)-5} more\")\n",
    "    else:\n",
    "        print(\"\\n✗ No IP directories found\")\n",
    "else:\n",
    "    print(f\"\\n✗ intermediate_models directory not found\")\n",
    "\n",
    "# Also check for any .zip IP packages\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"SEARCHING FOR IP PACKAGES (.zip)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "zip_files = list(build_path.rglob(\"*.zip\"))\n",
    "if zip_files:\n",
    "    print(f\"\\n✓ Found {len(zip_files)} .zip files:\")\n",
    "    for zf in zip_files[:10]:\n",
    "        rel_path = zf.relative_to(build_path)\n",
    "        size_kb = zf.stat().st_size / 1024\n",
    "        print(f\"  {rel_path} ({size_kb:.1f} KB)\")\n",
    "    if len(zip_files) > 10:\n",
    "        print(f\"  ... and {len(zip_files)-10} more\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No .zip IP packages found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0cffe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BUILD RESULTS ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "✓ Build directory: ../build\n",
      "\n",
      "⚠ No bitstream (.bit) files found\n",
      "\n",
      "✓ Resource Estimates (1):\n",
      "    report/estimate_layer_resources_hls.json\n",
      "\n",
      "✓ Intermediate Models (9):\n",
      "    step_apply_folding_config.onnx\n",
      "    step_convert_to_hw.onnx\n",
      "    step_hw_codegen.onnx\n",
      "    step_hw_ipgen.onnx\n",
      "    step_minimize_bit_width.onnx\n",
      "    ... and 4 more\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Analyze Build Results\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BUILD RESULTS ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    print(\"✗ Build directory not found!\")\n",
    "else:\n",
    "    print(f\"\\n✓ Build directory: {output_dir}\\n\")\n",
    "    \n",
    "    # Find key files\n",
    "    build_path = Path(output_dir)\n",
    "    \n",
    "    # 1. Bitstream\n",
    "    bitfiles = list(build_path.rglob(\"*.bit\"))\n",
    "    if bitfiles:\n",
    "        print(f\"✓ Bitstream Files ({len(bitfiles)}):\")\n",
    "        for bf in bitfiles:\n",
    "            size_mb = bf.stat().st_size / (1024*1024)\n",
    "            print(f\"    {bf.relative_to(build_path)} ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(\"⚠ No bitstream (.bit) files found\")\n",
    "    \n",
    "    # 2. Hardware handoff\n",
    "    hwh_files = list(build_path.rglob(\"*.hwh\"))\n",
    "    if hwh_files:\n",
    "        print(f\"\\n✓ Hardware Handoff Files ({len(hwh_files)}):\")\n",
    "        for hwh in hwh_files:\n",
    "            print(f\"    {hwh.relative_to(build_path)}\")\n",
    "    \n",
    "    # 3. Driver\n",
    "    driver_files = list(build_path.rglob(\"driver.py\"))\n",
    "    if driver_files:\n",
    "        print(f\"\\n✓ Driver Files ({len(driver_files)}):\")\n",
    "        for df in driver_files:\n",
    "            print(f\"    {df.relative_to(build_path)}\")\n",
    "    \n",
    "    # 4. Reports\n",
    "    report_files = list(build_path.rglob(\"*estimate*.json\"))\n",
    "    if report_files:\n",
    "        print(f\"\\n✓ Resource Estimates ({len(report_files)}):\")\n",
    "        for rf in sorted(report_files)[:3]:\n",
    "            print(f\"    {rf.relative_to(build_path)}\")\n",
    "            try:\n",
    "                with open(rf) as f:\n",
    "                    data = json.load(f)\n",
    "                    if 'total' in data:\n",
    "                        print(f\"      LUT: {data['total'].get('LUT', 'N/A')}\")\n",
    "                        print(f\"      FF: {data['total'].get('FF', 'N/A')}\")\n",
    "                        print(f\"      BRAM: {data['total'].get('BRAM_18K', 'N/A')}\")\n",
    "                        print(f\"      DSP: {data['total'].get('DSP', 'N/A')}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # 5. Deployment package\n",
    "    deploy_dirs = [d for d in build_path.rglob(\"deploy*\") if d.is_dir()]\n",
    "    if deploy_dirs:\n",
    "        print(f\"\\n✓ Deployment Package:\")\n",
    "        for dd in deploy_dirs:\n",
    "            print(f\"    {dd.relative_to(build_path)}/\")\n",
    "            deploy_files = list(dd.iterdir())[:5]\n",
    "            for df in deploy_files:\n",
    "                print(f\"      - {df.name}\")\n",
    "    \n",
    "    # 6. Intermediate models\n",
    "    intermediate_models = list(build_path.rglob(\"intermediate_models/*.onnx\"))\n",
    "    if intermediate_models:\n",
    "        print(f\"\\n✓ Intermediate Models ({len(intermediate_models)}):\")\n",
    "        for im in sorted(intermediate_models)[:5]:\n",
    "            print(f\"    {im.name}\")\n",
    "        if len(intermediate_models) > 5:\n",
    "            print(f\"    ... and {len(intermediate_models) - 5} more\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e306ccf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CREATING DEPLOYMENT PACKAGE FOR KV260\n",
      "======================================================================\n",
      "\n",
      "✓ Deployment directory: ../build/kv260_deployment\n",
      "\n",
      "  ⚠ No bitstream found\n",
      "  ⚠ No .hwh file found\n",
      "  ⚠ No driver found\n",
      "  ✓ Copied: resource_estimate.json\n",
      "  ✓ Created: README.md\n",
      "  ✓ Created: test_kv260.py\n",
      "\n",
      "✓ Deployment package complete: 3 files\n",
      "\n",
      "📦 Ready to deploy:\n",
      "   scp -r kv260_deployment xilinx@<kv260-ip>:~/\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Create Deployment Package for KV260\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREATING DEPLOYMENT PACKAGE FOR KV260\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create deployment directory\n",
    "deploy_output = Path(output_dir) / \"kv260_deployment\"\n",
    "deploy_output.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\n✓ Deployment directory: {deploy_output}\\n\")\n",
    "\n",
    "build_path = Path(output_dir)\n",
    "files_copied = 0\n",
    "\n",
    "# Copy bitstream\n",
    "bitfiles = list(build_path.rglob(\"*.bit\"))\n",
    "if bitfiles:\n",
    "    shutil.copy(bitfiles[0], deploy_output / \"finn_accel.bit\")\n",
    "    print(f\"  ✓ Copied: finn_accel.bit\")\n",
    "    files_copied += 1\n",
    "else:\n",
    "    print(f\"  ⚠ No bitstream found\")\n",
    "\n",
    "# Copy hardware handoff\n",
    "hwh_files = list(build_path.rglob(\"*.hwh\"))\n",
    "if hwh_files:\n",
    "    shutil.copy(hwh_files[0], deploy_output / \"finn_accel.hwh\")\n",
    "    print(f\"  ✓ Copied: finn_accel.hwh\")\n",
    "    files_copied += 1\n",
    "else:\n",
    "    print(f\"  ⚠ No .hwh file found\")\n",
    "\n",
    "# Copy driver\n",
    "driver_files = list(build_path.rglob(\"driver.py\"))\n",
    "if driver_files:\n",
    "    shutil.copy(driver_files[0], deploy_output / \"driver.py\")\n",
    "    print(f\"  ✓ Copied: driver.py\")\n",
    "    files_copied += 1\n",
    "else:\n",
    "    print(f\"  ⚠ No driver found\")\n",
    "\n",
    "# Copy reports\n",
    "report_files = list(build_path.rglob(\"*estimate*.json\"))\n",
    "if report_files:\n",
    "    shutil.copy(report_files[0], deploy_output / \"resource_estimate.json\")\n",
    "    print(f\"  ✓ Copied: resource_estimate.json\")\n",
    "    files_copied += 1\n",
    "\n",
    "# Create README\n",
    "readme = f\"\"\"# Ellipse Regression FINN Accelerator - Kria KV260 Deployment\n",
    "\n",
    "## Overview\n",
    "\n",
    "This package contains the FINN-generated accelerator for ellipse regression on Kria KV260.\n",
    "\n",
    "## Files\n",
    "\n",
    "- `finn_accel.bit` - Bitstream for Kria KV260\n",
    "- `finn_accel.hwh` - Hardware handoff file  \n",
    "- `driver.py` - Python driver code\n",
    "- `resource_estimate.json` - Resource utilization report\n",
    "\n",
    "## Deployment to KV260\n",
    "\n",
    "### 1. Copy Files to KV260\n",
    "\n",
    "```bash\n",
    "scp -r {deploy_output.name} xilinx@<kv260-ip>:~/\n",
    "```\n",
    "\n",
    "### 2. On KV260, Load the Overlay\n",
    "\n",
    "```python\n",
    "from pynq import Overlay\n",
    "\n",
    "# Load the accelerator\n",
    "overlay = Overlay(\"finn_accel.bit\")\n",
    "\n",
    "# Access the accelerator (adjust based on your driver)\n",
    "accel = overlay.finn_accel_0\n",
    "```\n",
    "\n",
    "### 3. Run Inference\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Prepare input data\n",
    "input_data = np.random.randint(0, 255, size=(1, 3, 224, 224), dtype=np.uint8)\n",
    "\n",
    "# Run inference\n",
    "output = accel.execute(input_data)\n",
    "\n",
    "print(\"Ellipse parameters:\", output)\n",
    "```\n",
    "\n",
    "## Build Information\n",
    "\n",
    "- **Target**: Kria KV260 (xck26-sfvc784-2LV-c)\n",
    "- **Clock**: 200 MHz\n",
    "- **Build date**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **FINN version**: Check build logs\n",
    "\n",
    "## Resource Usage\n",
    "\n",
    "See `resource_estimate.json` for detailed resource utilization.\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "1. **Overlay won't load**: Check that bitstream matches FPGA part\n",
    "2. **Driver import error**: Ensure PYNQ is installed on KV260\n",
    "3. **Execution hangs**: Check FIFO depths and DMA configuration\n",
    "\n",
    "## References\n",
    "\n",
    "- FINN Documentation: https://finn.readthedocs.io\n",
    "- Kria KV260: https://www.xilinx.com/products/som/kria/kv260-vision-starter-kit.html\n",
    "- PYNQ: http://www.pynq.io\n",
    "\"\"\"\n",
    "\n",
    "with open(deploy_output / \"README.md\", 'w') as f:\n",
    "    f.write(readme)\n",
    "print(f\"  ✓ Created: README.md\")\n",
    "files_copied += 1\n",
    "\n",
    "# Create a simple test script\n",
    "test_script = \"\"\"#!/usr/bin/env python3\n",
    "\\\"\\\"\\\"\n",
    "Simple test script for FINN accelerator on Kria KV260\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "import numpy as np\n",
    "from pynq import Overlay\n",
    "import time\n",
    "\n",
    "print(\"Loading FINN accelerator overlay...\")\n",
    "overlay = Overlay(\"finn_accel.bit\")\n",
    "print(\"✓ Overlay loaded\")\n",
    "\n",
    "# Get accelerator handle\n",
    "# Note: Adjust 'finn_accel_0' to match your actual IP name\n",
    "try:\n",
    "    accel = overlay.finn_accel_0\n",
    "    print(\"✓ Accelerator found\")\n",
    "except:\n",
    "    print(\"✗ Could not find accelerator IP\")\n",
    "    print(\"Available IPs:\", dir(overlay))\n",
    "    exit(1)\n",
    "\n",
    "# Prepare test input\n",
    "print(\"\\\\nPreparing test input...\")\n",
    "input_shape = (1, 3, 224, 224)  # Adjust to your model input\n",
    "input_data = np.random.randint(0, 255, size=input_shape, dtype=np.uint8)\n",
    "print(f\"  Input shape: {input_shape}\")\n",
    "\n",
    "# Run inference\n",
    "print(\"\\\\nRunning inference...\")\n",
    "start = time.time()\n",
    "output = accel.execute(input_data)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"✓ Inference complete in {elapsed*1000:.2f} ms\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Output: {output}\")\n",
    "\n",
    "print(\"\\\\n✓ Test completed successfully!\")\n",
    "\"\"\"\n",
    "\n",
    "with open(deploy_output / \"test_kv260.py\", 'w') as f:\n",
    "    f.write(test_script)\n",
    "print(f\"  ✓ Created: test_kv260.py\")\n",
    "files_copied += 1\n",
    "\n",
    "print(f\"\\n✓ Deployment package complete: {files_copied} files\")\n",
    "print(f\"\\n📦 Ready to deploy:\")\n",
    "print(f\"   scp -r {deploy_output.name} xilinx@<kv260-ip>:~/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ellipse-finn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
