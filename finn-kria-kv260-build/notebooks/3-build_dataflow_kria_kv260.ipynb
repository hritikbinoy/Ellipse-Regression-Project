{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd20ebd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ FINN_BUILD_DIR set to: /home/hritik/Desktop/Hritik/Project/ellipse-regression-project/finn-kria-kv260-build/build\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Set Environment Variables\n",
    "\n",
    "import os\n",
    "\n",
    "# Set FINN_BUILD_DIR to the build output directory\n",
    "os.environ[\"FINN_BUILD_DIR\"] = os.path.abspath(\"../build\")\n",
    "\n",
    "print(f\"✓ FINN_BUILD_DIR set to: {os.environ['FINN_BUILD_DIR']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f14663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1203 02:58:21.663359 12506 site-packages/torch/utils/cpp_extension.py:118] No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n",
      "  Python: 3.9.25 (main, Nov  3 2025, 22:33:05) \n",
      "[GCC 11.2.0]\n",
      "  Working directory: /home/hritik/Desktop/Hritik/Project/ellipse-regression-project/finn-kria-kv260-build/notebooks\n",
      "✓ FINN imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "# FINN is installed via pip, no need to add to path\n",
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "from finn.builder.build_dataflow_config import DataflowBuildConfig\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"  Python: {sys.version}\")\n",
    "print(f\"  Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Test FINN\n",
    "try:\n",
    "    from finn.util.basic import get_finn_root\n",
    "    print(f\"✓ FINN root: {get_finn_root()}\")\n",
    "except:\n",
    "    print(\"✓ FINN imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fcddcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded: ../../finn_build/ellipse_qonnx_streamlined.qonnx\n",
      "  Input: global_in [0, 1, 20, 20]\n",
      "  Output: global_out [1, 5]\n",
      "  Total nodes: 63\n",
      "\n",
      "  Node type distribution:\n",
      "    Mul: 12\n",
      "    Where: 12\n",
      "    Relu: 6\n",
      "    Round: 6\n",
      "    Greater: 6\n",
      "    Less: 6\n",
      "    Conv: 4\n",
      "    MaxPool: 4\n",
      "    Add: 3\n",
      "    Gemm: 3\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Verify Model Exists\n",
    "\n",
    "# Path to your streamlined QONNX model\n",
    "model_path = \"../../finn_build/ellipse_qonnx_streamlined.qonnx\"\n",
    "\n",
    "# Check if model exists\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Model not found: {model_path}\\n\"\n",
    "        \"Please run 2-finn.ipynb first to generate the streamlined model.\"\n",
    "    )\n",
    "\n",
    "# Load and inspect the model\n",
    "model = ModelWrapper(model_path)\n",
    "print(f\"✓ Model loaded: {model_path}\")\n",
    "print(f\"  Input: {model.graph.input[0].name} {[d.dim_value for d in model.graph.input[0].type.tensor_type.shape.dim]}\")\n",
    "print(f\"  Output: {model.graph.output[0].name} {[d.dim_value for d in model.graph.output[0].type.tensor_type.shape.dim]}\")\n",
    "print(f\"  Total nodes: {len(model.graph.node)}\")\n",
    "\n",
    "# Count node types\n",
    "from collections import Counter\n",
    "node_types = Counter([n.op_type for n in model.graph.node])\n",
    "print(f\"\\n  Node type distribution:\")\n",
    "for op_type, count in sorted(node_types.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"    {op_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da400f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Build configuration:\n",
      "  Model: ../../finn_build/ellipse_qonnx_streamlined.qonnx\n",
      "  Output: ../build\n",
      "  Target board: Kria KV260\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Configure Build Settings\n",
    "\n",
    "# Output directory for build artifacts\n",
    "output_dir = \"../build\"\n",
    "\n",
    "# Clean output directory if it exists\n",
    "if os.path.exists(output_dir):\n",
    "    print(f\"  Cleaning existing build directory: {output_dir}\")\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Build configuration:\")\n",
    "print(f\"  Model: {model_path}\")\n",
    "print(f\"  Output: {output_dir}\")\n",
    "print(f\"  Target board: Kria KV260\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81d91ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Build configuration created\n",
      "\n",
      "  Build steps (19):\n",
      "    1. step_qonnx_to_finn\n",
      "    2. step_tidy_up\n",
      "    3. step_streamline\n",
      "    4. step_convert_to_hw\n",
      "    5. step_create_dataflow_partition\n",
      "    6. step_specialize_layers\n",
      "    7. step_target_fps_parallelization\n",
      "    8. step_apply_folding_config\n",
      "    9. step_minimize_bit_width\n",
      "    10. step_generate_estimate_reports\n",
      "    11. step_hw_codegen\n",
      "    12. step_hw_ipgen\n",
      "    13. step_set_fifo_depths\n",
      "    14. step_create_stitched_ip\n",
      "    15. step_measure_rtlsim_performance\n",
      "    16. step_out_of_context_synthesis\n",
      "    17. step_synthesize_bitfile\n",
      "    18. step_make_pynq_driver\n",
      "    19. step_deployment_package\n",
      "\n",
      "  Output types:\n",
      "    - ESTIMATE_REPORTS\n",
      "    - STITCHED_IP\n",
      "    - RTLSIM_PERFORMANCE\n",
      "    - OOC_SYNTH\n",
      "    - BITFILE\n",
      "    - DEPLOYMENT_PACKAGE\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create Dataflow Build Configuration\n",
    "\n",
    "# Create a build config for the KV260 board\n",
    "cfg = DataflowBuildConfig(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # Target board\n",
    "    board=\"Kria-KV260-SOM\",  # Full board name for KV260\n",
    "    \n",
    "    # Clock frequency (200 MHz = 5.0 ns period)\n",
    "    synth_clk_period_ns=5.0,\n",
    "    \n",
    "    # Build steps - you can customize which steps to run\n",
    "    steps=build_cfg.default_build_dataflow_steps,\n",
    "    \n",
    "    # Resource optimization strategy\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,\n",
    "        build_cfg.DataflowOutputType.STITCHED_IP,\n",
    "        build_cfg.DataflowOutputType.RTLSIM_PERFORMANCE,\n",
    "        build_cfg.DataflowOutputType.OOC_SYNTH,\n",
    "        build_cfg.DataflowOutputType.BITFILE,\n",
    "        build_cfg.DataflowOutputType.DEPLOYMENT_PACKAGE,\n",
    "    ],\n",
    "    \n",
    "    # Performance settings\n",
    "    auto_fifo_depths=True,\n",
    "    auto_fifo_strategy=build_cfg.AutoFIFOSizingMethod.LARGEFIFO_RTLSIM,\n",
    "    \n",
    "    # Parallelization settings (adjust based on your model)\n",
    "    folding_config_file=None,  # Auto-generate folding config\n",
    "    \n",
    "    # Enable verbose output\n",
    "    verbose=True,\n",
    "    \n",
    "    # Save intermediate models\n",
    "    save_intermediate_models=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Build configuration created\")\n",
    "print(f\"\\n  Build steps ({len(cfg.steps)}):\")\n",
    "for i, step in enumerate(cfg.steps, 1):\n",
    "    print(f\"    {i}. {step}\")\n",
    "\n",
    "print(f\"\\n  Output types:\")\n",
    "for output_type in cfg.generate_outputs:\n",
    "    print(f\"    - {output_type.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ellipse-finn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
